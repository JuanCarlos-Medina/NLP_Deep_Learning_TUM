{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKPHEyuedRJQ"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNym3j0a7wv"
      },
      "source": [
        "This session will train faster with GPU!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI8KkFcoF9ui"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2o2rkuX1rjB"
      },
      "source": [
        "We'll use a language dataset provided by http://www.manythings.org/anki/ to translate from English to German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLnULuGeCuei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "919ed2bb-d2ca-464a-d157-a7024b44b0bc"
      },
      "source": [
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip\n",
        "\n",
        "lines = open('deu.txt', encoding='UTF-8').read().strip().split('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZO56LVL5eJK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b73c3b8-8c86-476c-af31-dd7fd0e4a890"
      },
      "source": [
        "lines[11]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Wait!\\tWarte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #2122378 (Pfirsichbaeumchen)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgsCQ3MlD6Yk"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc-4Pj1N6vu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "060bff15-5a39-4ee7-8487-c6a2732e45fa"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "  # This next line is confusing!\n",
        "  # We normalize unicode data, umlauts will be converted to normal letters\n",
        "  w = w.replace(\"ß\", \"ss\")\n",
        "  w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "sentence = \"May I borrow this book?\"\n",
        "print(preprocess_sentence(sentence))\n",
        "sentence = \"Über die Wolken.\"\n",
        "print(preprocess_sentence(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> uber die wolken . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtTbci8Z5qxy"
      },
      "source": [
        "english = []\n",
        "german = []\n",
        "for line in lines:\n",
        "  en = line.split('\\t')[0]\n",
        "  de = line.split('\\t')[1]\n",
        "  english.append(preprocess_sentence(en))\n",
        "  german.append(preprocess_sentence(de))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kDxHu0HbMJx"
      },
      "source": [
        "Using the complete dataset will probably kill the Google Colab notebook. Why? RAM problems! Either you reduce the number of data inputs or smaller batch size or smaller vocabulary (then take care of UNKowns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-RyjK9FVCi"
      },
      "source": [
        "NUM_EXAMPLES = 30000\n",
        "english = english[:NUM_EXAMPLES]\n",
        "german = german[:NUM_EXAMPLES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Ym1DPBiFET",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "e97750ca-2f34-4fe2-ed65-3441835f03e5"
      },
      "source": [
        "german[50:60]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> mach mit ! <end>',\n",
              " '<start> spring rein ! <end>',\n",
              " '<start> druck mich ! <end>',\n",
              " '<start> nimm mich in den arm ! <end>',\n",
              " '<start> umarme mich ! <end>',\n",
              " '<start> mir ist es wichtig . <end>',\n",
              " '<start> ich fiel . <end>',\n",
              " '<start> ich fiel hin . <end>',\n",
              " '<start> ich sturzte . <end>',\n",
              " '<start> ich bin hingefallen . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd9BFZZJDOJC"
      },
      "source": [
        "This time instead of using **TextVectorizer** to preprocess and tokenize the text, we are using our own **preprocess_sentence** function and then Keras [**Tokenizer**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
        "\n",
        "It's good that you get confortable with both types of layers, they have similar methods. For example, instead of adapt, Tokenizer uses fit_on_texts\n",
        "\n",
        "Disadvantage:\n",
        "TextVectorizer automatically pads to the longest sequence. For Tokenizer you have to do it on your own with **pad_sequences** method.\n",
        "\n",
        "Advantage:\n",
        "Tokenizer comes directly with **word_index** and **sequences_to_text** functions. We implemented these two on our own last week (Compare them!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH_HxNGlOUSS"
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(english)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(english)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "ge_tokenizer.fit_on_texts(german)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(german)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEeGIt7MOdHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e645236-048d-4baf-dbcd-e6224753c72e"
      },
      "source": [
        "data_en[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 34,  3,  2,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maPHGBpPOgTe"
      },
      "source": [
        "X_train,  X_test, Y_train, Y_test = train_test_split(data_en, data_ge, test_size=0.2)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "hidden_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28O3pz8vakHy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf0956f-2ec1-4b50-fb12-7dcd739d8d02"
      },
      "source": [
        "def max_len(sentence):\n",
        "    return max(len(s) for s in sentence)\n",
        "\n",
        "max_length_input = max_len(data_en)\n",
        "max_length_output = max_len(data_ge)  \n",
        "input_vocab_size = len(en_tokenizer.word_index) + 1  \n",
        "output_vocab_size = len(ge_tokenizer.word_index) + 1\n",
        "print(output_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCpTXqJGGauM"
      },
      "source": [
        "This time we shuffle and batch the dataset before the starting the training. It does not make a difference! I do this before this time, in order to check that the encoder-decoder layers are working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMY7pz95Opkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c53f7a64-160c-48ba-a554-ea66d7a8ae3c"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, \n",
        "                                                                                            drop_remainder=True)\n",
        "\n",
        "for example in dataset.take(1):\n",
        "  example_x, example_y = example\n",
        "print(example_x.shape) \n",
        "print(example_y.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 10)\n",
            "(64, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTe8LUmhV3KU"
      },
      "source": [
        "## Without Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isGc7qC4hVtT"
      },
      "source": [
        "We will use the Subclass API to create a Encoder and a Decoder Module.\n",
        "\n",
        "Without attention it is possible to use only the Functional API. However, to implement attention there is a [bug](https://github.com/tensorflow/addons/issues/1153) that only allows it through classes  :( \n",
        "\n",
        "From the beginning we will use the subclass API, otherwise the jump between no attention and attention is too big.\n",
        "\n",
        "If someone manages to transform the attention code to functional version. Please show me how :) In the [official documentation](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/seq2seq) it says that it is now working...\n",
        "\n",
        "It is nevertheless good to learn the Subclass API, as we will 100% need it when building a Transformer from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTvCUE07V-ie"
      },
      "source": [
        "# ENCODER\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims)\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=False, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "    def initialize_hidden_state(self): \n",
        "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
        "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
        "                                                               \n",
        "    def call(self, input, hidden_state):\n",
        "        embedding = self.embedding_layer(input)\n",
        "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
        "        return output, h_state, c_state\n",
        "\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_dims, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQL2ekvcJDF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f9d14de2-8b00-4b79-e749-f551a23c7db6"
      },
      "source": [
        "# Test  the encoder\n",
        "sample_initial_state = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
        "print(sample_output.shape)\n",
        "print(sample_h.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1024)\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9Zfv6hFFf-"
      },
      "source": [
        "We are going to use tensorflow addon for seq2seq models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNd1B7LXUGjB"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "# DECODER\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
        "   \n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    \n",
        "    self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.lstm_cell, \n",
        "                                            sampler=self.sampler, \n",
        "                                            output_layer=self.output_layer)\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    embedding = self.embedding(inputs)\n",
        "    # We will pass sequences without the <END> token, so the length is max length - 1\n",
        "    outputs, _, _ = self.decoder(embedding, initial_state=initial_state, \n",
        "                                 sequence_length=BATCH_SIZE*[max_length_output-1])\n",
        "    return outputs\n",
        "\n",
        "decoder = Decoder(output_vocab_size, embedding_dims, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-52OA4F5WlA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8c0ddd0-f0b1-40ac-a703-1e2ee25c4aeb"
      },
      "source": [
        "# Test the decoder\n",
        "sample_y = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "sample_decoder_output = decoder(sample_y, initial_state=[sample_h, sample_c])\n",
        "\n",
        "print(sample_decoder_output.rnn_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 12, 7262)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPB22kH9XEMX"
      },
      "source": [
        "Because we padded our sentences, we don't\n",
        "want to bias our results by considering equality of pad words between the labels\n",
        "and predictions. This custom loss function masks our predictions with the labels, so\n",
        "padded positions on the label are also removed from the predictions, and we only\n",
        "compute our loss using the non zero elements on both the label and predictions.\n",
        "\n",
        "The predicted Tensor has shape (BATCH_SIZE, max_length_output, output_vocab_size)\n",
        "\n",
        "The real Tensor has shape (BATCH_SIZE, max_length_output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZwj7ZxZCZR"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  # mask and loss have to have the same Tensor type\n",
        "  loss = mask * loss\n",
        "  loss = tf.reduce_mean(loss) # you need one loss scalar number for the mini batch\n",
        "  return loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBPiVrjKYpZN"
      },
      "source": [
        "We have to handle the training loop manually as well. Our train_step() function\n",
        "handles the flow of data and computes the loss at each step, applies the gradient\n",
        "of the loss back to the trainable weights, and returns the loss.\n",
        "\n",
        "\n",
        "These are quasi the same steps we took before with our example_x and example_y data. Try to understand these steps before "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wdxjo2raH6q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b366a04f-30a8-49bc-b498-9a692f169464"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  encoder_hidden = encoder.initialize_hidden_state() # Every epoch we use a zero Tensor matrix\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass the input through the encoder \n",
        "        encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "        decoder_input = target[ : , :-1 ] # ignore <end> token\n",
        "        real = target[ : , 1: ]           # ignore <start> token\n",
        "        # The encoder hidden state and the decoder input\n",
        "        # are passed to the decoder\n",
        "        decoder_output = decoder(decoder_input, [encoder_h, encoder_c]) \n",
        "        logits = decoder_output.rnn_output\n",
        "        batch_loss = loss_function(real, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(batch_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    epoch_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      epoch_loss / steps_per_epoch))\n",
        "  print('Time {:.4f} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.2367\n",
            "Epoch 1 Batch 100 Loss 1.9599\n",
            "Epoch 1 Batch 200 Loss 1.9536\n",
            "Epoch 1 Batch 300 Loss 1.8202\n",
            "Epoch 1 Loss 1.9803\n",
            "Time 76.0210 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6242\n",
            "Epoch 2 Batch 100 Loss 1.6441\n",
            "Epoch 2 Batch 200 Loss 1.6222\n",
            "Epoch 2 Batch 300 Loss 1.6233\n",
            "Epoch 2 Loss 1.6129\n",
            "Time 74.5426 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4970\n",
            "Epoch 3 Batch 100 Loss 1.4370\n",
            "Epoch 3 Batch 200 Loss 1.3742\n",
            "Epoch 3 Batch 300 Loss 1.3840\n",
            "Epoch 3 Loss 1.4619\n",
            "Time 74.3314 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3155\n",
            "Epoch 4 Batch 100 Loss 1.3850\n",
            "Epoch 4 Batch 200 Loss 1.3250\n",
            "Epoch 4 Batch 300 Loss 1.4406\n",
            "Epoch 4 Loss 1.3539\n",
            "Time 74.4977 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2954\n",
            "Epoch 5 Batch 100 Loss 1.3336\n",
            "Epoch 5 Batch 200 Loss 1.2161\n",
            "Epoch 5 Batch 300 Loss 1.2900\n",
            "Epoch 5 Loss 1.2684\n",
            "Time 75.9332 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.1367\n",
            "Epoch 6 Batch 100 Loss 1.1299\n",
            "Epoch 6 Batch 200 Loss 1.2074\n",
            "Epoch 6 Batch 300 Loss 1.0741\n",
            "Epoch 6 Loss 1.1272\n",
            "Time 75.0640 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0356\n",
            "Epoch 7 Batch 100 Loss 1.0060\n",
            "Epoch 7 Batch 200 Loss 1.0077\n",
            "Epoch 7 Batch 300 Loss 0.9566\n",
            "Epoch 7 Loss 0.9839\n",
            "Time 74.9274 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8983\n",
            "Epoch 8 Batch 100 Loss 0.8783\n",
            "Epoch 8 Batch 200 Loss 0.8365\n",
            "Epoch 8 Batch 300 Loss 0.8939\n",
            "Epoch 8 Loss 0.8801\n",
            "Time 74.9008 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8216\n",
            "Epoch 9 Batch 100 Loss 0.7977\n",
            "Epoch 9 Batch 200 Loss 0.7828\n",
            "Epoch 9 Batch 300 Loss 0.7592\n",
            "Epoch 9 Loss 0.7852\n",
            "Time 75.8130 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6886\n",
            "Epoch 10 Batch 100 Loss 0.6600\n",
            "Epoch 10 Batch 200 Loss 0.7084\n",
            "Epoch 10 Batch 300 Loss 0.6839\n",
            "Epoch 10 Loss 0.6950\n",
            "Time 75.5490 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6380\n",
            "Epoch 11 Batch 100 Loss 0.5845\n",
            "Epoch 11 Batch 200 Loss 0.6214\n",
            "Epoch 11 Batch 300 Loss 0.6014\n",
            "Epoch 11 Loss 0.6140\n",
            "Time 75.0576 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5098\n",
            "Epoch 12 Batch 100 Loss 0.5215\n",
            "Epoch 12 Batch 200 Loss 0.5567\n",
            "Epoch 12 Batch 300 Loss 0.5507\n",
            "Epoch 12 Loss 0.5421\n",
            "Time 74.9328 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4657\n",
            "Epoch 13 Batch 100 Loss 0.5243\n",
            "Epoch 13 Batch 200 Loss 0.4557\n",
            "Epoch 13 Batch 300 Loss 0.4261\n",
            "Epoch 13 Loss 0.4778\n",
            "Time 76.2544 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.3759\n",
            "Epoch 14 Batch 100 Loss 0.4110\n",
            "Epoch 14 Batch 200 Loss 0.4256\n",
            "Epoch 14 Batch 300 Loss 0.4422\n",
            "Epoch 14 Loss 0.4192\n",
            "Time 76.1227 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.3288\n",
            "Epoch 15 Batch 100 Loss 0.3581\n",
            "Epoch 15 Batch 200 Loss 0.3645\n",
            "Epoch 15 Batch 300 Loss 0.3943\n",
            "Epoch 15 Loss 0.3641\n",
            "Time 75.8903 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3115\n",
            "Epoch 16 Batch 100 Loss 0.2839\n",
            "Epoch 16 Batch 200 Loss 0.3053\n",
            "Epoch 16 Batch 300 Loss 0.3665\n",
            "Epoch 16 Loss 0.3154\n",
            "Time 75.7363 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2269\n",
            "Epoch 17 Batch 100 Loss 0.2864\n",
            "Epoch 17 Batch 200 Loss 0.2875\n",
            "Epoch 17 Batch 300 Loss 0.2823\n",
            "Epoch 17 Loss 0.2726\n",
            "Time 76.4242 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1782\n",
            "Epoch 18 Batch 100 Loss 0.2409\n",
            "Epoch 18 Batch 200 Loss 0.2097\n",
            "Epoch 18 Batch 300 Loss 0.2743\n",
            "Epoch 18 Loss 0.2335\n",
            "Time 76.2722 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1688\n",
            "Epoch 19 Batch 100 Loss 0.1795\n",
            "Epoch 19 Batch 200 Loss 0.2006\n",
            "Epoch 19 Batch 300 Loss 0.2036\n",
            "Epoch 19 Loss 0.1991\n",
            "Time 75.9881 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1586\n",
            "Epoch 20 Batch 100 Loss 0.1615\n",
            "Epoch 20 Batch 200 Loss 0.2025\n",
            "Epoch 20 Batch 300 Loss 0.2061\n",
            "Epoch 20 Loss 0.1721\n",
            "Time 76.0542 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1167\n",
            "Epoch 21 Batch 100 Loss 0.1366\n",
            "Epoch 21 Batch 200 Loss 0.1569\n",
            "Epoch 21 Batch 300 Loss 0.1347\n",
            "Epoch 21 Loss 0.1502\n",
            "Time 76.9068 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0995\n",
            "Epoch 22 Batch 100 Loss 0.0901\n",
            "Epoch 22 Batch 200 Loss 0.1302\n",
            "Epoch 22 Batch 300 Loss 0.1466\n",
            "Epoch 22 Loss 0.1333\n",
            "Time 76.7722 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0781\n",
            "Epoch 23 Batch 100 Loss 0.0865\n",
            "Epoch 23 Batch 200 Loss 0.1421\n",
            "Epoch 23 Batch 300 Loss 0.1216\n",
            "Epoch 23 Loss 0.1201\n",
            "Time 75.9160 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0918\n",
            "Epoch 24 Batch 100 Loss 0.0932\n",
            "Epoch 24 Batch 200 Loss 0.1153\n",
            "Epoch 24 Batch 300 Loss 0.1301\n",
            "Epoch 24 Loss 0.1109\n",
            "Time 76.1827 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0912\n",
            "Epoch 25 Batch 100 Loss 0.0960\n",
            "Epoch 25 Batch 200 Loss 0.0759\n",
            "Epoch 25 Batch 300 Loss 0.1047\n",
            "Epoch 25 Loss 0.1035\n",
            "Time 76.5438 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0894\n",
            "Epoch 26 Batch 100 Loss 0.0799\n",
            "Epoch 26 Batch 200 Loss 0.0942\n",
            "Epoch 26 Batch 300 Loss 0.1058\n",
            "Epoch 26 Loss 0.0976\n",
            "Time 77.6455 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0694\n",
            "Epoch 27 Batch 100 Loss 0.0629\n",
            "Epoch 27 Batch 200 Loss 0.1118\n",
            "Epoch 27 Batch 300 Loss 0.1049\n",
            "Epoch 27 Loss 0.0930\n",
            "Time 76.8744 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0808\n",
            "Epoch 28 Batch 100 Loss 0.0825\n",
            "Epoch 28 Batch 200 Loss 0.1090\n",
            "Epoch 28 Batch 300 Loss 0.0864\n",
            "Epoch 28 Loss 0.0889\n",
            "Time 76.6251 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0570\n",
            "Epoch 29 Batch 100 Loss 0.0905\n",
            "Epoch 29 Batch 200 Loss 0.0739\n",
            "Epoch 29 Batch 300 Loss 0.0962\n",
            "Epoch 29 Loss 0.0854\n",
            "Time 76.3827 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0675\n",
            "Epoch 30 Batch 100 Loss 0.1033\n",
            "Epoch 30 Batch 200 Loss 0.0796\n",
            "Epoch 30 Batch 300 Loss 0.0709\n",
            "Epoch 30 Loss 0.0836\n",
            "Time 78.5323 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0642\n",
            "Epoch 31 Batch 100 Loss 0.0715\n",
            "Epoch 31 Batch 200 Loss 0.0767\n",
            "Epoch 31 Batch 300 Loss 0.1399\n",
            "Epoch 31 Loss 0.0815\n",
            "Time 77.4512 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0722\n",
            "Epoch 32 Batch 100 Loss 0.0604\n",
            "Epoch 32 Batch 200 Loss 0.0743\n",
            "Epoch 32 Batch 300 Loss 0.0880\n",
            "Epoch 32 Loss 0.0798\n",
            "Time 78.3301 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0663\n",
            "Epoch 33 Batch 100 Loss 0.0657\n",
            "Epoch 33 Batch 200 Loss 0.0816\n",
            "Epoch 33 Batch 300 Loss 0.0847\n",
            "Epoch 33 Loss 0.0784\n",
            "Time 78.2640 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0512\n",
            "Epoch 34 Batch 100 Loss 0.0739\n",
            "Epoch 34 Batch 200 Loss 0.0794\n",
            "Epoch 34 Batch 300 Loss 0.0735\n",
            "Epoch 34 Loss 0.0768\n",
            "Time 79.4334 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0641\n",
            "Epoch 35 Batch 100 Loss 0.0606\n",
            "Epoch 35 Batch 200 Loss 0.0634\n",
            "Epoch 35 Batch 300 Loss 0.0678\n",
            "Epoch 35 Loss 0.0754\n",
            "Time 78.5554 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0503\n",
            "Epoch 36 Batch 100 Loss 0.0770\n",
            "Epoch 36 Batch 200 Loss 0.0807\n",
            "Epoch 36 Batch 300 Loss 0.0936\n",
            "Epoch 36 Loss 0.0735\n",
            "Time 77.1497 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0700\n",
            "Epoch 37 Batch 100 Loss 0.0518\n",
            "Epoch 37 Batch 200 Loss 0.0756\n",
            "Epoch 37 Batch 300 Loss 0.0610\n",
            "Epoch 37 Loss 0.0723\n",
            "Time 78.7480 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0596\n",
            "Epoch 38 Batch 100 Loss 0.0635\n",
            "Epoch 38 Batch 200 Loss 0.1038\n",
            "Epoch 38 Batch 300 Loss 0.0903\n",
            "Epoch 38 Loss 0.0711\n",
            "Time 79.3317 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0464\n",
            "Epoch 39 Batch 100 Loss 0.0435\n",
            "Epoch 39 Batch 200 Loss 0.0760\n",
            "Epoch 39 Batch 300 Loss 0.0969\n",
            "Epoch 39 Loss 0.0706\n",
            "Time 78.4552 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0430\n",
            "Epoch 40 Batch 100 Loss 0.0458\n",
            "Epoch 40 Batch 200 Loss 0.0832\n",
            "Epoch 40 Batch 300 Loss 0.0743\n",
            "Epoch 40 Loss 0.0699\n",
            "Time 78.7046 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0642\n",
            "Epoch 41 Batch 100 Loss 0.0441\n",
            "Epoch 41 Batch 200 Loss 0.0435\n",
            "Epoch 41 Batch 300 Loss 0.0836\n",
            "Epoch 41 Loss 0.0689\n",
            "Time 79.7489 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0659\n",
            "Epoch 42 Batch 100 Loss 0.0569\n",
            "Epoch 42 Batch 200 Loss 0.0539\n",
            "Epoch 42 Batch 300 Loss 0.0592\n",
            "Epoch 42 Loss 0.0678\n",
            "Time 79.5082 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0394\n",
            "Epoch 43 Batch 100 Loss 0.0468\n",
            "Epoch 43 Batch 200 Loss 0.0838\n",
            "Epoch 43 Batch 300 Loss 0.0817\n",
            "Epoch 43 Loss 0.0668\n",
            "Time 79.4240 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0447\n",
            "Epoch 44 Batch 100 Loss 0.0584\n",
            "Epoch 44 Batch 200 Loss 0.0618\n",
            "Epoch 44 Batch 300 Loss 0.0666\n",
            "Epoch 44 Loss 0.0659\n",
            "Time 79.4086 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0562\n",
            "Epoch 45 Batch 100 Loss 0.0406\n",
            "Epoch 45 Batch 200 Loss 0.0723\n",
            "Epoch 45 Batch 300 Loss 0.0715\n",
            "Epoch 45 Loss 0.0645\n",
            "Time 81.3093 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0393\n",
            "Epoch 46 Batch 100 Loss 0.0697\n",
            "Epoch 46 Batch 200 Loss 0.0631\n",
            "Epoch 46 Batch 300 Loss 0.0574\n",
            "Epoch 46 Loss 0.0640\n",
            "Time 80.2179 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0566\n",
            "Epoch 47 Batch 100 Loss 0.0573\n",
            "Epoch 47 Batch 200 Loss 0.0544\n",
            "Epoch 47 Batch 300 Loss 0.0941\n",
            "Epoch 47 Loss 0.0645\n",
            "Time 79.4770 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0463\n",
            "Epoch 48 Batch 100 Loss 0.0522\n",
            "Epoch 48 Batch 200 Loss 0.0584\n",
            "Epoch 48 Batch 300 Loss 0.0824\n",
            "Epoch 48 Loss 0.0644\n",
            "Time 79.9963 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0468\n",
            "Epoch 49 Batch 100 Loss 0.0604\n",
            "Epoch 49 Batch 200 Loss 0.0571\n",
            "Epoch 49 Batch 300 Loss 0.0747\n",
            "Epoch 49 Loss 0.0630\n",
            "Time 81.5200 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0553\n",
            "Epoch 50 Batch 100 Loss 0.0365\n",
            "Epoch 50 Batch 200 Loss 0.0705\n",
            "Epoch 50 Batch 300 Loss 0.0737\n",
            "Epoch 50 Loss 0.0625\n",
            "Time 80.1075 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0433\n",
            "Epoch 51 Batch 100 Loss 0.0466\n",
            "Epoch 51 Batch 200 Loss 0.0606\n",
            "Epoch 51 Batch 300 Loss 0.0704\n",
            "Epoch 51 Loss 0.0608\n",
            "Time 79.9651 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0443\n",
            "Epoch 52 Batch 100 Loss 0.0633\n",
            "Epoch 52 Batch 200 Loss 0.0698\n",
            "Epoch 52 Batch 300 Loss 0.0738\n",
            "Epoch 52 Loss 0.0602\n",
            "Time 81.2520 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0415\n",
            "Epoch 53 Batch 100 Loss 0.0600\n",
            "Epoch 53 Batch 200 Loss 0.0704\n",
            "Epoch 53 Batch 300 Loss 0.0722\n",
            "Epoch 53 Loss 0.0596\n",
            "Time 81.7925 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0397\n",
            "Epoch 54 Batch 100 Loss 0.0596\n",
            "Epoch 54 Batch 200 Loss 0.0534\n",
            "Epoch 54 Batch 300 Loss 0.0586\n",
            "Epoch 54 Loss 0.0595\n",
            "Time 80.8211 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0441\n",
            "Epoch 55 Batch 100 Loss 0.0492\n",
            "Epoch 55 Batch 200 Loss 0.0583\n",
            "Epoch 55 Batch 300 Loss 0.0720\n",
            "Epoch 55 Loss 0.0590\n",
            "Time 80.6997 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0308\n",
            "Epoch 56 Batch 100 Loss 0.0461\n",
            "Epoch 56 Batch 200 Loss 0.0767\n",
            "Epoch 56 Batch 300 Loss 0.0632\n",
            "Epoch 56 Loss 0.0588\n",
            "Time 82.5020 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0299\n",
            "Epoch 57 Batch 100 Loss 0.0360\n",
            "Epoch 57 Batch 200 Loss 0.0815\n",
            "Epoch 57 Batch 300 Loss 0.0818\n",
            "Epoch 57 Loss 0.0586\n",
            "Time 83.7411 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0558\n",
            "Epoch 58 Batch 100 Loss 0.0624\n",
            "Epoch 58 Batch 200 Loss 0.0416\n",
            "Epoch 58 Batch 300 Loss 0.0765\n",
            "Epoch 58 Loss 0.0578\n",
            "Time 82.3666 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0593\n",
            "Epoch 59 Batch 100 Loss 0.0654\n",
            "Epoch 59 Batch 200 Loss 0.0499\n",
            "Epoch 59 Batch 300 Loss 0.0799\n",
            "Epoch 59 Loss 0.0587\n",
            "Time 82.9134 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0598\n",
            "Epoch 60 Batch 100 Loss 0.0496\n",
            "Epoch 60 Batch 200 Loss 0.0714\n",
            "Epoch 60 Batch 300 Loss 0.0702\n",
            "Epoch 60 Loss 0.0579\n",
            "Time 83.0399 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0468\n",
            "Epoch 61 Batch 100 Loss 0.0440\n",
            "Epoch 61 Batch 200 Loss 0.0411\n",
            "Epoch 61 Batch 300 Loss 0.0726\n",
            "Epoch 61 Loss 0.0571\n",
            "Time 84.8270 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0385\n",
            "Epoch 62 Batch 100 Loss 0.0360\n",
            "Epoch 62 Batch 200 Loss 0.0573\n",
            "Epoch 62 Batch 300 Loss 0.0678\n",
            "Epoch 62 Loss 0.0559\n",
            "Time 84.0554 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0316\n",
            "Epoch 63 Batch 100 Loss 0.0403\n",
            "Epoch 63 Batch 200 Loss 0.0479\n",
            "Epoch 63 Batch 300 Loss 0.0724\n",
            "Epoch 63 Loss 0.0547\n",
            "Time 83.4256 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0332\n",
            "Epoch 64 Batch 100 Loss 0.0441\n",
            "Epoch 64 Batch 200 Loss 0.0518\n",
            "Epoch 64 Batch 300 Loss 0.0619\n",
            "Epoch 64 Loss 0.0540\n",
            "Time 85.6361 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0486\n",
            "Epoch 65 Batch 100 Loss 0.0368\n",
            "Epoch 65 Batch 200 Loss 0.0649\n",
            "Epoch 65 Batch 300 Loss 0.0354\n",
            "Epoch 65 Loss 0.0534\n",
            "Time 85.3284 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0255\n",
            "Epoch 66 Batch 100 Loss 0.0365\n",
            "Epoch 66 Batch 200 Loss 0.0573\n",
            "Epoch 66 Batch 300 Loss 0.0436\n",
            "Epoch 66 Loss 0.0531\n",
            "Time 85.4294 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0296\n",
            "Epoch 67 Batch 100 Loss 0.0597\n",
            "Epoch 67 Batch 200 Loss 0.0624\n",
            "Epoch 67 Batch 300 Loss 0.0671\n",
            "Epoch 67 Loss 0.0533\n",
            "Time 85.6706 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0418\n",
            "Epoch 68 Batch 100 Loss 0.0525\n",
            "Epoch 68 Batch 200 Loss 0.0513\n",
            "Epoch 68 Batch 300 Loss 0.0609\n",
            "Epoch 68 Loss 0.0532\n",
            "Time 86.3109 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0384\n",
            "Epoch 69 Batch 100 Loss 0.0407\n",
            "Epoch 69 Batch 200 Loss 0.0516\n",
            "Epoch 69 Batch 300 Loss 0.0583\n",
            "Epoch 69 Loss 0.0536\n",
            "Time 85.3010 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0458\n",
            "Epoch 70 Batch 100 Loss 0.0648\n",
            "Epoch 70 Batch 200 Loss 0.0482\n",
            "Epoch 70 Batch 300 Loss 0.0645\n",
            "Epoch 70 Loss 0.0536\n",
            "Time 85.3931 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0519\n",
            "Epoch 71 Batch 100 Loss 0.0462\n",
            "Epoch 71 Batch 200 Loss 0.0345\n",
            "Epoch 71 Batch 300 Loss 0.0861\n",
            "Epoch 71 Loss 0.0530\n",
            "Time 85.9310 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0336\n",
            "Epoch 72 Batch 100 Loss 0.0462\n",
            "Epoch 72 Batch 200 Loss 0.0676\n",
            "Epoch 72 Batch 300 Loss 0.0567\n",
            "Epoch 72 Loss 0.0527\n",
            "Time 86.6740 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0357\n",
            "Epoch 73 Batch 100 Loss 0.0379\n",
            "Epoch 73 Batch 200 Loss 0.0393\n",
            "Epoch 73 Batch 300 Loss 0.0672\n",
            "Epoch 73 Loss 0.0520\n",
            "Time 85.5592 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0476\n",
            "Epoch 74 Batch 100 Loss 0.0434\n",
            "Epoch 74 Batch 200 Loss 0.0500\n",
            "Epoch 74 Batch 300 Loss 0.0634\n",
            "Epoch 74 Loss 0.0513\n",
            "Time 85.7030 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0520\n",
            "Epoch 75 Batch 100 Loss 0.0308\n",
            "Epoch 75 Batch 200 Loss 0.0467\n",
            "Epoch 75 Batch 300 Loss 0.0564\n",
            "Epoch 75 Loss 0.0514\n",
            "Time 87.4368 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0414\n",
            "Epoch 76 Batch 100 Loss 0.0427\n",
            "Epoch 76 Batch 200 Loss 0.0476\n",
            "Epoch 76 Batch 300 Loss 0.0464\n",
            "Epoch 76 Loss 0.0518\n",
            "Time 85.9237 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0453\n",
            "Epoch 77 Batch 100 Loss 0.0519\n",
            "Epoch 77 Batch 200 Loss 0.0517\n",
            "Epoch 77 Batch 300 Loss 0.0469\n",
            "Epoch 77 Loss 0.0529\n",
            "Time 85.7588 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0981\n",
            "Epoch 78 Batch 100 Loss 0.0405\n",
            "Epoch 78 Batch 200 Loss 0.0540\n",
            "Epoch 78 Batch 300 Loss 0.0707\n",
            "Epoch 78 Loss 0.0525\n",
            "Time 86.2175 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0342\n",
            "Epoch 79 Batch 100 Loss 0.0385\n",
            "Epoch 79 Batch 200 Loss 0.0594\n",
            "Epoch 79 Batch 300 Loss 0.0570\n",
            "Epoch 79 Loss 0.0519\n",
            "Time 87.0114 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0235\n",
            "Epoch 80 Batch 100 Loss 0.0435\n",
            "Epoch 80 Batch 200 Loss 0.0635\n",
            "Epoch 80 Batch 300 Loss 0.0515\n",
            "Epoch 80 Loss 0.0494\n",
            "Time 86.2553 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0401\n",
            "Epoch 81 Batch 100 Loss 0.0486\n",
            "Epoch 81 Batch 200 Loss 0.0555\n",
            "Epoch 81 Batch 300 Loss 0.0549\n",
            "Epoch 81 Loss 0.0484\n",
            "Time 85.9561 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0276\n",
            "Epoch 82 Batch 100 Loss 0.0431\n",
            "Epoch 82 Batch 200 Loss 0.0564\n",
            "Epoch 82 Batch 300 Loss 0.0493\n",
            "Epoch 82 Loss 0.0475\n",
            "Time 87.8667 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0330\n",
            "Epoch 83 Batch 100 Loss 0.0301\n",
            "Epoch 83 Batch 200 Loss 0.0463\n",
            "Epoch 83 Batch 300 Loss 0.0530\n",
            "Epoch 83 Loss 0.0471\n",
            "Time 86.8827 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0449\n",
            "Epoch 84 Batch 100 Loss 0.0531\n",
            "Epoch 84 Batch 200 Loss 0.0626\n",
            "Epoch 84 Batch 300 Loss 0.0665\n",
            "Epoch 84 Loss 0.0475\n",
            "Time 86.8216 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0390\n",
            "Epoch 85 Batch 100 Loss 0.0309\n",
            "Epoch 85 Batch 200 Loss 0.0499\n",
            "Epoch 85 Batch 300 Loss 0.0328\n",
            "Epoch 85 Loss 0.0475\n",
            "Time 87.7131 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0280\n",
            "Epoch 86 Batch 100 Loss 0.0306\n",
            "Epoch 86 Batch 200 Loss 0.0320\n",
            "Epoch 86 Batch 300 Loss 0.0387\n",
            "Epoch 86 Loss 0.0473\n",
            "Time 88.5567 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0337\n",
            "Epoch 87 Batch 100 Loss 0.0487\n",
            "Epoch 87 Batch 200 Loss 0.0582\n",
            "Epoch 87 Batch 300 Loss 0.0514\n",
            "Epoch 87 Loss 0.0476\n",
            "Time 87.4472 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0229\n",
            "Epoch 88 Batch 100 Loss 0.0488\n",
            "Epoch 88 Batch 200 Loss 0.0595\n",
            "Epoch 88 Batch 300 Loss 0.0547\n",
            "Epoch 88 Loss 0.0483\n",
            "Time 87.1301 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0401\n",
            "Epoch 89 Batch 100 Loss 0.0573\n",
            "Epoch 89 Batch 200 Loss 0.0484\n",
            "Epoch 89 Batch 300 Loss 0.0497\n",
            "Epoch 89 Loss 0.0488\n",
            "Time 88.4688 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0314\n",
            "Epoch 90 Batch 100 Loss 0.0398\n",
            "Epoch 90 Batch 200 Loss 0.0648\n",
            "Epoch 90 Batch 300 Loss 0.0551\n",
            "Epoch 90 Loss 0.0506\n",
            "Time 88.0043 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0342\n",
            "Epoch 91 Batch 100 Loss 0.0417\n",
            "Epoch 91 Batch 200 Loss 0.0370\n",
            "Epoch 91 Batch 300 Loss 0.0605\n",
            "Epoch 91 Loss 0.0515\n",
            "Time 87.4552 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0257\n",
            "Epoch 92 Batch 100 Loss 0.0306\n",
            "Epoch 92 Batch 200 Loss 0.0424\n",
            "Epoch 92 Batch 300 Loss 0.0514\n",
            "Epoch 92 Loss 0.0504\n",
            "Time 88.1483 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0279\n",
            "Epoch 93 Batch 100 Loss 0.0357\n",
            "Epoch 93 Batch 200 Loss 0.0467\n",
            "Epoch 93 Batch 300 Loss 0.0499\n",
            "Epoch 93 Loss 0.0486\n",
            "Time 88.8983 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0373\n",
            "Epoch 94 Batch 100 Loss 0.0418\n",
            "Epoch 94 Batch 200 Loss 0.0463\n",
            "Epoch 94 Batch 300 Loss 0.0435\n",
            "Epoch 94 Loss 0.0464\n",
            "Time 87.7548 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0386\n",
            "Epoch 95 Batch 100 Loss 0.0320\n",
            "Epoch 95 Batch 200 Loss 0.0483\n",
            "Epoch 95 Batch 300 Loss 0.0574\n",
            "Epoch 95 Loss 0.0455\n",
            "Time 88.0854 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0302\n",
            "Epoch 96 Batch 100 Loss 0.0277\n",
            "Epoch 96 Batch 200 Loss 0.0372\n",
            "Epoch 96 Batch 300 Loss 0.0637\n",
            "Epoch 96 Loss 0.0453\n",
            "Time 88.6314 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0291\n",
            "Epoch 97 Batch 100 Loss 0.0380\n",
            "Epoch 97 Batch 200 Loss 0.0565\n",
            "Epoch 97 Batch 300 Loss 0.0412\n",
            "Epoch 97 Loss 0.0449\n",
            "Time 89.5019 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0362\n",
            "Epoch 98 Batch 100 Loss 0.0444\n",
            "Epoch 98 Batch 200 Loss 0.0502\n",
            "Epoch 98 Batch 300 Loss 0.0478\n",
            "Epoch 98 Loss 0.0446\n",
            "Time 88.7040 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0466\n",
            "Epoch 99 Batch 100 Loss 0.0332\n",
            "Epoch 99 Batch 200 Loss 0.0475\n",
            "Epoch 99 Batch 300 Loss 0.0510\n",
            "Epoch 99 Loss 0.0445\n",
            "Time 89.0919 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0172\n",
            "Epoch 100 Batch 100 Loss 0.0368\n",
            "Epoch 100 Batch 200 Loss 0.0405\n",
            "Epoch 100 Batch 300 Loss 0.0509\n",
            "Epoch 100 Loss 0.0444\n",
            "Time 89.7196 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_iKTawxhE8g"
      },
      "source": [
        "**Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXntTPfacEu5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc02f63d-eee8-4d10-e671-7f537252b270"
      },
      "source": [
        "def translate(sentence, preprocess=True):\n",
        "    if preprocess:\n",
        "        sentence = preprocess_sentence(sentence)\n",
        "        sentence_tokens = en_tokenizer.texts_to_sequences([sentence])\n",
        "        input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=max_length_input, padding='post')\n",
        "    else:\n",
        "        input = sentence\n",
        "    input = tf.convert_to_tensor(input)\n",
        "\n",
        "    encoder_hidden = [tf.zeros((1, hidden_units)), tf.zeros((1, hidden_units))]\n",
        "    encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "\n",
        "    ### This time we use the greedy sampler because we want the word with the highest probability!\n",
        "    ### We are not generating new text, where a probability sampling would be better\n",
        "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "    # Instantiate a BasicDecoder object\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.lstm_cell, \n",
        "                                                sampler=greedy_sampler, \n",
        "                                                output_layer=decoder.output_layer)\n",
        "\n",
        "    ### Since the BasicDecoder wraps around Decoder's lstm cell only, you have to ensure that the inputs to BasicDecoder \n",
        "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] \n",
        "    ### and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "    # Additionally, we give the start token to the decoder, and also the end token, so that it stops translating\n",
        "    start_token = tf.convert_to_tensor([ge_tokenizer.word_index['<start>']])\n",
        "    end_token = ge_tokenizer.word_index['<end>']\n",
        "\n",
        "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, \n",
        "                                     end_token= end_token, initial_state=[encoder_h, encoder_c])\n",
        "\n",
        "    result_sequence  = outputs.sample_id.numpy()\n",
        "    return ge_tokenizer.sequences_to_texts(result_sequence)[0]\n",
        "\n",
        "translate(\"I love you!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich liebe dich ! <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lghhGBC0vsNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64e82968-009d-428b-c19c-c23a78c0cfd6"
      },
      "source": [
        "translate(\"I want to kiss you!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich mochte dich kussen . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK-uiiPfv5I0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32304614-72f2-488d-b171-f722dfd24be2"
      },
      "source": [
        "translate(\"I played the piano today\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich habe das auto gekauft . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOXClLznwDnG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad9014d9-b3fe-4920-81d3-b913d5bce09a"
      },
      "source": [
        "translate(\"The teacher was happy to train the language model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'das licht war aus . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0thYyxWgx4B"
      },
      "source": [
        "[**BLEU Scores**](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnF2jTw9xMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a010d4cf-3ca6-4aea-d826-21275bfbc8bc"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "bleu_scores = []\n",
        "smooth_fn = SmoothingFunction()\n",
        "\n",
        "for input, target in zip(X_test, Y_test):\n",
        "    original = ge_tokenizer.sequences_to_texts([target])[0]\n",
        "    predicted = translate([input], preprocess=False)\n",
        "    original = re.sub(\"(<end>)|(<start>)|\\?|!|\\.\", \"\", original)\n",
        "    predicted = re.sub(\"(<end>)|\\?|!|\\.\", \"\", predicted)\n",
        "    original_tokens = original.strip().split(\" \")\n",
        "    predicted_tokens = predicted.strip().split(\" \")\n",
        "    score = sentence_bleu([original_tokens], predicted_tokens, \n",
        "                          smoothing_function=smooth_fn.method1)\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "np.mean(np.array(bleu_scores)) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.307577949992638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TChhDgk4Qnn4"
      },
      "source": [
        "## With Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drcztamGd1l3"
      },
      "source": [
        "The Encoder stays almost the same. Only the LSTM layer now needs to return the hidden states at every input to pass it to attention.\n",
        "For this we activate return_state=True. [Read here](https://medium.com/@sanjivgautamofficial/lstm-in-keras-56a59264c0b2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LRsKeTSvm-k"
      },
      "source": [
        "class EncoderAttention(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims)\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=True, \n",
        "                                                     return_state=True ) # We need the lstm outputs \n",
        "                                                                         # to calculate attention!\n",
        "    \n",
        "    def initialize_hidden_state(self): \n",
        "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
        "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
        "                                                               \n",
        "    def call(self, input, hidden_state):\n",
        "        embedding = self.embedding_layer(input)\n",
        "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
        "        return output, h_state, c_state\n",
        "\n",
        "\n",
        "encoder = EncoderAttention(input_vocab_size, embedding_dims, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUULtQDAZ-p3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "239eec0e-e706-482d-b40d-a0de4b146391"
      },
      "source": [
        "# Test  the encoder\n",
        "sample_initial_state = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
        "print(sample_output.shape)\n",
        "print(sample_h.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 10, 1024)\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a434Vf9lRb4q"
      },
      "source": [
        "The Decoder is the one that changes the most. I comment with \"#N\", the new changes needed. For all future steps, wee need to initialize the attention and then pass the initial state (encoder output) through the attention cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbe9xv_BaRYE"
      },
      "source": [
        "class DecoderAttention(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
        "   \n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    self.attention_mechanism = tfa.seq2seq.LuongAttention(hidden_units, memory_sequence_length=BATCH_SIZE*[max_length_input]) #N\n",
        "    \n",
        "    self.attention_cell = tfa.seq2seq.AttentionWrapper(cell=self.lstm_cell, # N\n",
        "                                  attention_mechanism=self.attention_mechanism, \n",
        "                                  attention_layer_size=hidden_units)\n",
        "    \n",
        "    self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.attention_cell, # N\n",
        "                                            sampler=self.sampler, \n",
        "                                            output_layer=self.output_layer)\n",
        "\n",
        "  def build_initial_state(self, batch_size, encoder_state): #N\n",
        "    decoder_initial_state = self.attention_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    embedding = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(embedding, initial_state=initial_state, sequence_length=BATCH_SIZE*[max_length_output-1])\n",
        "    return outputs\n",
        "\n",
        "decoder = DecoderAttention(output_vocab_size, embedding_dims, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLMj6b_ywvyd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de5393ae-1b01-4300-a378-cc5b7e1afc52"
      },
      "source": [
        "# Test the decoder\n",
        "sample_y = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "decoder.attention_mechanism.setup_memory(sample_output) # Attention needs the last output of the Encoder\n",
        "                                                        # as starting point\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c]) # N\n",
        "\n",
        "\n",
        "sample_decoder_output = decoder(sample_y, initial_state)\n",
        "\n",
        "print(sample_decoder_output.rnn_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 12, 7262)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2zmnPc_cfxP"
      },
      "source": [
        "Same loss function as before! No changes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZnPD0bZwToW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e5a9a55-fc41-4b01-a81a-aecfb83c2f88"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  encoder_hidden = encoder.initialize_hidden_state() # Every epoch we use a zero Tensor matrix\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass the input through the encoder \n",
        "        encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "        decoder_input = target[ : , :-1 ] # Ignore <end> token\n",
        "        real = target[ : , 1: ]         # ignore <start> token\n",
        "        # The encoder output, encoder hidden state and the decoder input\n",
        "        # is passed to the decoder\n",
        "        decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
        "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [encoder_h, encoder_c]) # N\n",
        "        decoder_output = decoder(decoder_input, decoder_initial_state) \n",
        "        logits = decoder_output.rnn_output\n",
        "        batch_loss = loss_function(real, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(batch_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    epoch_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      epoch_loss / steps_per_epoch))\n",
        "  print('Time {:.4f} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5146\n",
            "Epoch 1 Batch 100 Loss 2.9734\n",
            "Epoch 1 Batch 200 Loss 1.9706\n",
            "Epoch 1 Batch 300 Loss 2.0473\n",
            "Epoch 1 Loss 2.8581\n",
            "Time 111.3372 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8199\n",
            "Epoch 2 Batch 100 Loss 1.5814\n",
            "Epoch 2 Batch 200 Loss 1.7211\n",
            "Epoch 2 Batch 300 Loss 1.5967\n",
            "Epoch 2 Loss 1.6797\n",
            "Time 111.2661 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6079\n",
            "Epoch 3 Batch 100 Loss 1.5383\n",
            "Epoch 3 Batch 200 Loss 1.4252\n",
            "Epoch 3 Batch 300 Loss 1.6349\n",
            "Epoch 3 Loss 1.5118\n",
            "Time 111.4935 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3876\n",
            "Epoch 4 Batch 100 Loss 1.3024\n",
            "Epoch 4 Batch 200 Loss 1.3459\n",
            "Epoch 4 Batch 300 Loss 1.4257\n",
            "Epoch 4 Loss 1.4006\n",
            "Time 110.7898 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2880\n",
            "Epoch 5 Batch 100 Loss 1.3369\n",
            "Epoch 5 Batch 200 Loss 1.2947\n",
            "Epoch 5 Batch 300 Loss 1.3020\n",
            "Epoch 5 Loss 1.3138\n",
            "Time 111.7957 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2069\n",
            "Epoch 6 Batch 100 Loss 1.1938\n",
            "Epoch 6 Batch 200 Loss 1.2285\n",
            "Epoch 6 Batch 300 Loss 1.1304\n",
            "Epoch 6 Loss 1.2177\n",
            "Time 111.7171 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1337\n",
            "Epoch 7 Batch 100 Loss 1.0426\n",
            "Epoch 7 Batch 200 Loss 1.0465\n",
            "Epoch 7 Batch 300 Loss 1.2226\n",
            "Epoch 7 Loss 1.1263\n",
            "Time 111.1954 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0756\n",
            "Epoch 8 Batch 100 Loss 1.0844\n",
            "Epoch 8 Batch 200 Loss 0.9962\n",
            "Epoch 8 Batch 300 Loss 1.1728\n",
            "Epoch 8 Loss 1.0573\n",
            "Time 112.7408 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0567\n",
            "Epoch 9 Batch 100 Loss 0.8789\n",
            "Epoch 9 Batch 200 Loss 0.9631\n",
            "Epoch 9 Batch 300 Loss 1.0513\n",
            "Epoch 9 Loss 0.9900\n",
            "Time 112.5878 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8265\n",
            "Epoch 10 Batch 100 Loss 0.9440\n",
            "Epoch 10 Batch 200 Loss 0.8565\n",
            "Epoch 10 Batch 300 Loss 0.9236\n",
            "Epoch 10 Loss 0.9268\n",
            "Time 110.0193 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8483\n",
            "Epoch 11 Batch 100 Loss 0.8116\n",
            "Epoch 11 Batch 200 Loss 0.8411\n",
            "Epoch 11 Batch 300 Loss 0.8905\n",
            "Epoch 11 Loss 0.8691\n",
            "Time 111.1744 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.8003\n",
            "Epoch 12 Batch 100 Loss 0.7673\n",
            "Epoch 12 Batch 200 Loss 0.8203\n",
            "Epoch 12 Batch 300 Loss 0.7896\n",
            "Epoch 12 Loss 0.8134\n",
            "Time 111.6929 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.6689\n",
            "Epoch 13 Batch 100 Loss 0.7749\n",
            "Epoch 13 Batch 200 Loss 0.7482\n",
            "Epoch 13 Batch 300 Loss 0.8073\n",
            "Epoch 13 Loss 0.7596\n",
            "Time 111.9373 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6408\n",
            "Epoch 14 Batch 100 Loss 0.6888\n",
            "Epoch 14 Batch 200 Loss 0.7735\n",
            "Epoch 14 Batch 300 Loss 0.7230\n",
            "Epoch 14 Loss 0.7116\n",
            "Time 108.0641 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.6173\n",
            "Epoch 15 Batch 100 Loss 0.6323\n",
            "Epoch 15 Batch 200 Loss 0.6436\n",
            "Epoch 15 Batch 300 Loss 0.6278\n",
            "Epoch 15 Loss 0.6626\n",
            "Time 107.7568 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6537\n",
            "Epoch 16 Batch 100 Loss 0.6036\n",
            "Epoch 16 Batch 200 Loss 0.6399\n",
            "Epoch 16 Batch 300 Loss 0.6193\n",
            "Epoch 16 Loss 0.6170\n",
            "Time 108.4975 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5522\n",
            "Epoch 17 Batch 100 Loss 0.4915\n",
            "Epoch 17 Batch 200 Loss 0.6138\n",
            "Epoch 17 Batch 300 Loss 0.6067\n",
            "Epoch 17 Loss 0.5739\n",
            "Time 108.4095 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4551\n",
            "Epoch 18 Batch 100 Loss 0.4426\n",
            "Epoch 18 Batch 200 Loss 0.5497\n",
            "Epoch 18 Batch 300 Loss 0.5744\n",
            "Epoch 18 Loss 0.5355\n",
            "Time 108.4887 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4423\n",
            "Epoch 19 Batch 100 Loss 0.4431\n",
            "Epoch 19 Batch 200 Loss 0.5169\n",
            "Epoch 19 Batch 300 Loss 0.5569\n",
            "Epoch 19 Loss 0.5021\n",
            "Time 109.0361 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4610\n",
            "Epoch 20 Batch 100 Loss 0.4264\n",
            "Epoch 20 Batch 200 Loss 0.5072\n",
            "Epoch 20 Batch 300 Loss 0.5316\n",
            "Epoch 20 Loss 0.4667\n",
            "Time 109.9088 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.3604\n",
            "Epoch 21 Batch 100 Loss 0.3803\n",
            "Epoch 21 Batch 200 Loss 0.4234\n",
            "Epoch 21 Batch 300 Loss 0.5409\n",
            "Epoch 21 Loss 0.4374\n",
            "Time 109.9049 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.4082\n",
            "Epoch 22 Batch 100 Loss 0.4268\n",
            "Epoch 22 Batch 200 Loss 0.4240\n",
            "Epoch 22 Batch 300 Loss 0.4514\n",
            "Epoch 22 Loss 0.4087\n",
            "Time 111.3767 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.4030\n",
            "Epoch 23 Batch 100 Loss 0.4225\n",
            "Epoch 23 Batch 200 Loss 0.3674\n",
            "Epoch 23 Batch 300 Loss 0.3694\n",
            "Epoch 23 Loss 0.3832\n",
            "Time 110.7496 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.3821\n",
            "Epoch 24 Batch 100 Loss 0.3510\n",
            "Epoch 24 Batch 200 Loss 0.3483\n",
            "Epoch 24 Batch 300 Loss 0.3236\n",
            "Epoch 24 Loss 0.3649\n",
            "Time 110.3867 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.3296\n",
            "Epoch 25 Batch 100 Loss 0.2986\n",
            "Epoch 25 Batch 200 Loss 0.3626\n",
            "Epoch 25 Batch 300 Loss 0.3323\n",
            "Epoch 25 Loss 0.3468\n",
            "Time 111.3441 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.2922\n",
            "Epoch 26 Batch 100 Loss 0.2699\n",
            "Epoch 26 Batch 200 Loss 0.3097\n",
            "Epoch 26 Batch 300 Loss 0.4463\n",
            "Epoch 26 Loss 0.3547\n",
            "Time 110.1908 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.3783\n",
            "Epoch 27 Batch 100 Loss 0.3286\n",
            "Epoch 27 Batch 200 Loss 0.2921\n",
            "Epoch 27 Batch 300 Loss 0.3284\n",
            "Epoch 27 Loss 0.3394\n",
            "Time 110.8119 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.2121\n",
            "Epoch 28 Batch 100 Loss 0.2818\n",
            "Epoch 28 Batch 200 Loss 0.2846\n",
            "Epoch 28 Batch 300 Loss 0.3169\n",
            "Epoch 28 Loss 0.2998\n",
            "Time 112.8357 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.2530\n",
            "Epoch 29 Batch 100 Loss 0.2935\n",
            "Epoch 29 Batch 200 Loss 0.2732\n",
            "Epoch 29 Batch 300 Loss 0.2488\n",
            "Epoch 29 Loss 0.2793\n",
            "Time 112.9014 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.2802\n",
            "Epoch 30 Batch 100 Loss 0.2799\n",
            "Epoch 30 Batch 200 Loss 0.2436\n",
            "Epoch 30 Batch 300 Loss 0.2765\n",
            "Epoch 30 Loss 0.2720\n",
            "Time 113.9205 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.2533\n",
            "Epoch 31 Batch 100 Loss 0.1971\n",
            "Epoch 31 Batch 200 Loss 0.2677\n",
            "Epoch 31 Batch 300 Loss 0.2793\n",
            "Epoch 31 Loss 0.2632\n",
            "Time 113.1384 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.2400\n",
            "Epoch 32 Batch 100 Loss 0.1668\n",
            "Epoch 32 Batch 200 Loss 0.2253\n",
            "Epoch 32 Batch 300 Loss 0.3036\n",
            "Epoch 32 Loss 0.2515\n",
            "Time 113.8941 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.2059\n",
            "Epoch 33 Batch 100 Loss 0.2334\n",
            "Epoch 33 Batch 200 Loss 0.2766\n",
            "Epoch 33 Batch 300 Loss 0.2795\n",
            "Epoch 33 Loss 0.2485\n",
            "Time 114.5742 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.2148\n",
            "Epoch 34 Batch 100 Loss 0.2064\n",
            "Epoch 34 Batch 200 Loss 0.1844\n",
            "Epoch 34 Batch 300 Loss 0.2378\n",
            "Epoch 34 Loss 0.2372\n",
            "Time 113.2274 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.2442\n",
            "Epoch 35 Batch 100 Loss 0.1909\n",
            "Epoch 35 Batch 200 Loss 0.2186\n",
            "Epoch 35 Batch 300 Loss 0.2521\n",
            "Epoch 35 Loss 0.2256\n",
            "Time 113.4943 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.1736\n",
            "Epoch 36 Batch 100 Loss 0.1817\n",
            "Epoch 36 Batch 200 Loss 0.2023\n",
            "Epoch 36 Batch 300 Loss 0.1800\n",
            "Epoch 36 Loss 0.2223\n",
            "Time 114.8830 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.1788\n",
            "Epoch 37 Batch 100 Loss 0.1810\n",
            "Epoch 37 Batch 200 Loss 0.1856\n",
            "Epoch 37 Batch 300 Loss 0.2395\n",
            "Epoch 37 Loss 0.2125\n",
            "Time 114.1602 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.1410\n",
            "Epoch 38 Batch 100 Loss 0.1622\n",
            "Epoch 38 Batch 200 Loss 0.2065\n",
            "Epoch 38 Batch 300 Loss 0.2145\n",
            "Epoch 38 Loss 0.2038\n",
            "Time 118.2922 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.1520\n",
            "Epoch 39 Batch 100 Loss 0.1370\n",
            "Epoch 39 Batch 200 Loss 0.2040\n",
            "Epoch 39 Batch 300 Loss 0.2334\n",
            "Epoch 39 Loss 0.1984\n",
            "Time 113.3593 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.1734\n",
            "Epoch 40 Batch 100 Loss 0.1727\n",
            "Epoch 40 Batch 200 Loss 0.2301\n",
            "Epoch 40 Batch 300 Loss 0.2094\n",
            "Epoch 40 Loss 0.1962\n",
            "Time 112.4155 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.1616\n",
            "Epoch 41 Batch 100 Loss 0.1452\n",
            "Epoch 41 Batch 200 Loss 0.1949\n",
            "Epoch 41 Batch 300 Loss 0.2307\n",
            "Epoch 41 Loss 0.1921\n",
            "Time 114.0569 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.1780\n",
            "Epoch 42 Batch 100 Loss 0.2084\n",
            "Epoch 42 Batch 200 Loss 0.1748\n",
            "Epoch 42 Batch 300 Loss 0.1691\n",
            "Epoch 42 Loss 0.1847\n",
            "Time 111.6841 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.1255\n",
            "Epoch 43 Batch 100 Loss 0.1571\n",
            "Epoch 43 Batch 200 Loss 0.1580\n",
            "Epoch 43 Batch 300 Loss 0.1825\n",
            "Epoch 43 Loss 0.1773\n",
            "Time 111.3685 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.1148\n",
            "Epoch 44 Batch 100 Loss 0.1430\n",
            "Epoch 44 Batch 200 Loss 0.1761\n",
            "Epoch 44 Batch 300 Loss 0.2199\n",
            "Epoch 44 Loss 0.1727\n",
            "Time 112.4714 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.1455\n",
            "Epoch 45 Batch 100 Loss 0.1671\n",
            "Epoch 45 Batch 200 Loss 0.1802\n",
            "Epoch 45 Batch 300 Loss 0.1538\n",
            "Epoch 45 Loss 0.1755\n",
            "Time 111.8457 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.1185\n",
            "Epoch 46 Batch 100 Loss 0.1305\n",
            "Epoch 46 Batch 200 Loss 0.1371\n",
            "Epoch 46 Batch 300 Loss 0.1601\n",
            "Epoch 46 Loss 0.1658\n",
            "Time 112.5685 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.1292\n",
            "Epoch 47 Batch 100 Loss 0.1149\n",
            "Epoch 47 Batch 200 Loss 0.1877\n",
            "Epoch 47 Batch 300 Loss 0.1491\n",
            "Epoch 47 Loss 0.1600\n",
            "Time 116.4274 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.1121\n",
            "Epoch 48 Batch 100 Loss 0.1824\n",
            "Epoch 48 Batch 200 Loss 0.1603\n",
            "Epoch 48 Batch 300 Loss 0.1901\n",
            "Epoch 48 Loss 0.1578\n",
            "Time 114.2170 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.1690\n",
            "Epoch 49 Batch 100 Loss 0.1551\n",
            "Epoch 49 Batch 200 Loss 0.1617\n",
            "Epoch 49 Batch 300 Loss 0.1743\n",
            "Epoch 49 Loss 0.1575\n",
            "Time 114.7461 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.1572\n",
            "Epoch 50 Batch 100 Loss 0.1601\n",
            "Epoch 50 Batch 200 Loss 0.1363\n",
            "Epoch 50 Batch 300 Loss 0.1519\n",
            "Epoch 50 Loss 0.1530\n",
            "Time 114.3613 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.1139\n",
            "Epoch 51 Batch 100 Loss 0.1433\n",
            "Epoch 51 Batch 200 Loss 0.1779\n",
            "Epoch 51 Batch 300 Loss 0.2253\n",
            "Epoch 51 Loss 0.1536\n",
            "Time 115.3776 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.1128\n",
            "Epoch 52 Batch 100 Loss 0.1110\n",
            "Epoch 52 Batch 200 Loss 0.1008\n",
            "Epoch 52 Batch 300 Loss 0.2231\n",
            "Epoch 52 Loss 0.1525\n",
            "Time 119.8820 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.1406\n",
            "Epoch 53 Batch 100 Loss 0.1424\n",
            "Epoch 53 Batch 200 Loss 0.1623\n",
            "Epoch 53 Batch 300 Loss 0.1563\n",
            "Epoch 53 Loss 0.1485\n",
            "Time 119.5658 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.1236\n",
            "Epoch 54 Batch 100 Loss 0.1758\n",
            "Epoch 54 Batch 200 Loss 0.1457\n",
            "Epoch 54 Batch 300 Loss 0.1546\n",
            "Epoch 54 Loss 0.1465\n",
            "Time 119.0458 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0925\n",
            "Epoch 55 Batch 100 Loss 0.1203\n",
            "Epoch 55 Batch 200 Loss 0.1711\n",
            "Epoch 55 Batch 300 Loss 0.1372\n",
            "Epoch 55 Loss 0.1439\n",
            "Time 120.5549 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.1100\n",
            "Epoch 56 Batch 100 Loss 0.1415\n",
            "Epoch 56 Batch 200 Loss 0.1454\n",
            "Epoch 56 Batch 300 Loss 0.1219\n",
            "Epoch 56 Loss 0.1427\n",
            "Time 117.6543 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0818\n",
            "Epoch 57 Batch 100 Loss 0.1171\n",
            "Epoch 57 Batch 200 Loss 0.1115\n",
            "Epoch 57 Batch 300 Loss 0.1388\n",
            "Epoch 57 Loss 0.1369\n",
            "Time 119.0335 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.1220\n",
            "Epoch 58 Batch 100 Loss 0.1285\n",
            "Epoch 58 Batch 200 Loss 0.1127\n",
            "Epoch 58 Batch 300 Loss 0.1480\n",
            "Epoch 58 Loss 0.1360\n",
            "Time 114.8543 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.1023\n",
            "Epoch 59 Batch 100 Loss 0.0897\n",
            "Epoch 59 Batch 200 Loss 0.1165\n",
            "Epoch 59 Batch 300 Loss 0.1929\n",
            "Epoch 59 Loss 0.1347\n",
            "Time 114.8643 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.1175\n",
            "Epoch 60 Batch 100 Loss 0.1215\n",
            "Epoch 60 Batch 200 Loss 0.1384\n",
            "Epoch 60 Batch 300 Loss 0.1130\n",
            "Epoch 60 Loss 0.1339\n",
            "Time 114.7417 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.1279\n",
            "Epoch 61 Batch 100 Loss 0.1073\n",
            "Epoch 61 Batch 200 Loss 0.1083\n",
            "Epoch 61 Batch 300 Loss 0.1118\n",
            "Epoch 61 Loss 0.1352\n",
            "Time 114.6956 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.1005\n",
            "Epoch 62 Batch 100 Loss 0.1067\n",
            "Epoch 62 Batch 200 Loss 0.1455\n",
            "Epoch 62 Batch 300 Loss 0.1334\n",
            "Epoch 62 Loss 0.1348\n",
            "Time 114.0794 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0719\n",
            "Epoch 63 Batch 100 Loss 0.1586\n",
            "Epoch 63 Batch 200 Loss 0.1580\n",
            "Epoch 63 Batch 300 Loss 0.1730\n",
            "Epoch 63 Loss 0.1326\n",
            "Time 115.9167 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0793\n",
            "Epoch 64 Batch 100 Loss 0.1324\n",
            "Epoch 64 Batch 200 Loss 0.1348\n",
            "Epoch 64 Batch 300 Loss 0.1451\n",
            "Epoch 64 Loss 0.1321\n",
            "Time 115.3941 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.1175\n",
            "Epoch 65 Batch 100 Loss 0.1264\n",
            "Epoch 65 Batch 200 Loss 0.1440\n",
            "Epoch 65 Batch 300 Loss 0.1293\n",
            "Epoch 65 Loss 0.1263\n",
            "Time 116.6213 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0919\n",
            "Epoch 66 Batch 100 Loss 0.0976\n",
            "Epoch 66 Batch 200 Loss 0.0986\n",
            "Epoch 66 Batch 300 Loss 0.1572\n",
            "Epoch 66 Loss 0.1274\n",
            "Time 115.5902 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1246\n",
            "Epoch 67 Batch 100 Loss 0.1298\n",
            "Epoch 67 Batch 200 Loss 0.1384\n",
            "Epoch 67 Batch 300 Loss 0.1282\n",
            "Epoch 67 Loss 0.1268\n",
            "Time 115.6206 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0895\n",
            "Epoch 68 Batch 100 Loss 0.1477\n",
            "Epoch 68 Batch 200 Loss 0.0825\n",
            "Epoch 68 Batch 300 Loss 0.1112\n",
            "Epoch 68 Loss 0.1233\n",
            "Time 116.7695 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1240\n",
            "Epoch 69 Batch 100 Loss 0.1297\n",
            "Epoch 69 Batch 200 Loss 0.1679\n",
            "Epoch 69 Batch 300 Loss 0.1580\n",
            "Epoch 69 Loss 0.1253\n",
            "Time 115.1156 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0872\n",
            "Epoch 70 Batch 100 Loss 0.0692\n",
            "Epoch 70 Batch 200 Loss 0.1173\n",
            "Epoch 70 Batch 300 Loss 0.1386\n",
            "Epoch 70 Loss 0.1246\n",
            "Time 114.6625 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1294\n",
            "Epoch 71 Batch 100 Loss 0.0889\n",
            "Epoch 71 Batch 200 Loss 0.1191\n",
            "Epoch 71 Batch 300 Loss 0.1267\n",
            "Epoch 71 Loss 0.1216\n",
            "Time 115.5350 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0803\n",
            "Epoch 72 Batch 100 Loss 0.1189\n",
            "Epoch 72 Batch 200 Loss 0.0969\n",
            "Epoch 72 Batch 300 Loss 0.1672\n",
            "Epoch 72 Loss 0.1176\n",
            "Time 114.5613 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0687\n",
            "Epoch 73 Batch 100 Loss 0.1326\n",
            "Epoch 73 Batch 200 Loss 0.1270\n",
            "Epoch 73 Batch 300 Loss 0.1517\n",
            "Epoch 73 Loss 0.1193\n",
            "Time 115.1622 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0677\n",
            "Epoch 74 Batch 100 Loss 0.0937\n",
            "Epoch 74 Batch 200 Loss 0.1127\n",
            "Epoch 74 Batch 300 Loss 0.1519\n",
            "Epoch 74 Loss 0.1200\n",
            "Time 114.7968 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0734\n",
            "Epoch 75 Batch 100 Loss 0.1313\n",
            "Epoch 75 Batch 200 Loss 0.1723\n",
            "Epoch 75 Batch 300 Loss 0.0901\n",
            "Epoch 75 Loss 0.1139\n",
            "Time 115.6312 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0916\n",
            "Epoch 76 Batch 100 Loss 0.1337\n",
            "Epoch 76 Batch 200 Loss 0.1357\n",
            "Epoch 76 Batch 300 Loss 0.0982\n",
            "Epoch 76 Loss 0.1141\n",
            "Time 116.0970 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0698\n",
            "Epoch 77 Batch 100 Loss 0.0599\n",
            "Epoch 77 Batch 200 Loss 0.1314\n",
            "Epoch 77 Batch 300 Loss 0.1277\n",
            "Epoch 77 Loss 0.1133\n",
            "Time 115.3589 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0644\n",
            "Epoch 78 Batch 100 Loss 0.0746\n",
            "Epoch 78 Batch 200 Loss 0.1368\n",
            "Epoch 78 Batch 300 Loss 0.1006\n",
            "Epoch 78 Loss 0.1134\n",
            "Time 114.8517 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0921\n",
            "Epoch 79 Batch 100 Loss 0.1107\n",
            "Epoch 79 Batch 200 Loss 0.0804\n",
            "Epoch 79 Batch 300 Loss 0.1569\n",
            "Epoch 79 Loss 0.1169\n",
            "Time 116.0041 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0762\n",
            "Epoch 80 Batch 100 Loss 0.1233\n",
            "Epoch 80 Batch 200 Loss 0.1002\n",
            "Epoch 80 Batch 300 Loss 0.1312\n",
            "Epoch 80 Loss 0.1145\n",
            "Time 114.9951 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0696\n",
            "Epoch 81 Batch 100 Loss 0.0712\n",
            "Epoch 81 Batch 200 Loss 0.1453\n",
            "Epoch 81 Batch 300 Loss 0.1664\n",
            "Epoch 81 Loss 0.1143\n",
            "Time 115.6080 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0951\n",
            "Epoch 82 Batch 100 Loss 0.1357\n",
            "Epoch 82 Batch 200 Loss 0.1041\n",
            "Epoch 82 Batch 300 Loss 0.1245\n",
            "Epoch 82 Loss 0.1129\n",
            "Time 116.0056 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0732\n",
            "Epoch 83 Batch 100 Loss 0.0780\n",
            "Epoch 83 Batch 200 Loss 0.1281\n",
            "Epoch 83 Batch 300 Loss 0.1434\n",
            "Epoch 83 Loss 0.1109\n",
            "Time 116.2112 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0801\n",
            "Epoch 84 Batch 100 Loss 0.1363\n",
            "Epoch 84 Batch 200 Loss 0.1050\n",
            "Epoch 84 Batch 300 Loss 0.1230\n",
            "Epoch 84 Loss 0.1099\n",
            "Time 117.1262 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0978\n",
            "Epoch 85 Batch 100 Loss 0.0902\n",
            "Epoch 85 Batch 200 Loss 0.1356\n",
            "Epoch 85 Batch 300 Loss 0.1176\n",
            "Epoch 85 Loss 0.1117\n",
            "Time 116.2526 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0874\n",
            "Epoch 86 Batch 100 Loss 0.0803\n",
            "Epoch 86 Batch 200 Loss 0.0944\n",
            "Epoch 86 Batch 300 Loss 0.1214\n",
            "Epoch 86 Loss 0.1100\n",
            "Time 116.5999 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0841\n",
            "Epoch 87 Batch 100 Loss 0.1018\n",
            "Epoch 87 Batch 200 Loss 0.1005\n",
            "Epoch 87 Batch 300 Loss 0.1278\n",
            "Epoch 87 Loss 0.1071\n",
            "Time 117.4586 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0802\n",
            "Epoch 88 Batch 100 Loss 0.0931\n",
            "Epoch 88 Batch 200 Loss 0.0698\n",
            "Epoch 88 Batch 300 Loss 0.0810\n",
            "Epoch 88 Loss 0.1055\n",
            "Time 116.9610 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0935\n",
            "Epoch 89 Batch 100 Loss 0.1007\n",
            "Epoch 89 Batch 200 Loss 0.0994\n",
            "Epoch 89 Batch 300 Loss 0.1141\n",
            "Epoch 89 Loss 0.1043\n",
            "Time 117.4930 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0837\n",
            "Epoch 90 Batch 100 Loss 0.1010\n",
            "Epoch 90 Batch 200 Loss 0.1301\n",
            "Epoch 90 Batch 300 Loss 0.1453\n",
            "Epoch 90 Loss 0.1056\n",
            "Time 117.0935 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0908\n",
            "Epoch 91 Batch 100 Loss 0.0675\n",
            "Epoch 91 Batch 200 Loss 0.1083\n",
            "Epoch 91 Batch 300 Loss 0.1001\n",
            "Epoch 91 Loss 0.1044\n",
            "Time 117.0979 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0803\n",
            "Epoch 92 Batch 100 Loss 0.0829\n",
            "Epoch 92 Batch 200 Loss 0.0961\n",
            "Epoch 92 Batch 300 Loss 0.1192\n",
            "Epoch 92 Loss 0.1045\n",
            "Time 118.1817 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0890\n",
            "Epoch 93 Batch 100 Loss 0.1183\n",
            "Epoch 93 Batch 200 Loss 0.1266\n",
            "Epoch 93 Batch 300 Loss 0.1536\n",
            "Epoch 93 Loss 0.1052\n",
            "Time 117.5754 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1082\n",
            "Epoch 94 Batch 100 Loss 0.0966\n",
            "Epoch 94 Batch 200 Loss 0.1065\n",
            "Epoch 94 Batch 300 Loss 0.0942\n",
            "Epoch 94 Loss 0.1036\n",
            "Time 118.2282 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1036\n",
            "Epoch 95 Batch 100 Loss 0.1149\n",
            "Epoch 95 Batch 200 Loss 0.0850\n",
            "Epoch 95 Batch 300 Loss 0.1565\n",
            "Epoch 95 Loss 0.1055\n",
            "Time 119.0986 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0733\n",
            "Epoch 96 Batch 100 Loss 0.0796\n",
            "Epoch 96 Batch 200 Loss 0.1065\n",
            "Epoch 96 Batch 300 Loss 0.1430\n",
            "Epoch 96 Loss 0.1042\n",
            "Time 119.2436 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0977\n",
            "Epoch 97 Batch 100 Loss 0.1108\n",
            "Epoch 97 Batch 200 Loss 0.1259\n",
            "Epoch 97 Batch 300 Loss 0.1283\n",
            "Epoch 97 Loss 0.1009\n",
            "Time 119.8659 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0853\n",
            "Epoch 98 Batch 100 Loss 0.0919\n",
            "Epoch 98 Batch 200 Loss 0.1134\n",
            "Epoch 98 Batch 300 Loss 0.1160\n",
            "Epoch 98 Loss 0.0989\n",
            "Time 120.0274 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0949\n",
            "Epoch 99 Batch 100 Loss 0.1040\n",
            "Epoch 99 Batch 200 Loss 0.1190\n",
            "Epoch 99 Batch 300 Loss 0.1223\n",
            "Epoch 99 Loss 0.0981\n",
            "Time 119.7960 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0639\n",
            "Epoch 100 Batch 100 Loss 0.1052\n",
            "Epoch 100 Batch 200 Loss 0.1263\n",
            "Epoch 100 Batch 300 Loss 0.0769\n",
            "Epoch 100 Loss 0.1001\n",
            "Time 120.6451 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I4Upg-Q4UHT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95fe902b-0a61-4b56-d7bc-dc6d3ddef931"
      },
      "source": [
        "def translate(sentence, preprocess=True):\n",
        "    if preprocess:\n",
        "        sentence = preprocess_sentence(sentence)\n",
        "        sentence_tokens = en_tokenizer.texts_to_sequences([sentence])\n",
        "        input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=max_length_input, padding='post')\n",
        "    else:\n",
        "        input = sentence\n",
        "    input = tf.convert_to_tensor(input)\n",
        "\n",
        "    encoder_hidden = [tf.zeros((1, hidden_units)), tf.zeros((1, hidden_units))]\n",
        "    encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "    start_token = tf.convert_to_tensor([ge_tokenizer.word_index['<start>']])\n",
        "    end_token = ge_tokenizer.word_index['<end>']\n",
        "\n",
        "    # This time we use the greedy sampler because we want the word with the highest probability!\n",
        "    # We are not generating new text, where a probability sampling would be better\n",
        "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "    # Instantiate a BasicDecoder object\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_cell, # N\n",
        "                                                sampler=greedy_sampler, output_layer=decoder.output_layer)\n",
        "    # Setup Memory in decoder stack\n",
        "    decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
        "\n",
        "    # set decoder_initial_state\n",
        "    decoder_initial_state = decoder.build_initial_state(batch_size=1, encoder_state=[encoder_h, encoder_c]) # N\n",
        "\n",
        "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, end_token= end_token, initial_state=decoder_initial_state)\n",
        "\n",
        "    result_sequence  = outputs.sample_id.numpy()\n",
        "    return ge_tokenizer.sequences_to_texts(result_sequence)[0]\n",
        "\n",
        "translate(\"I love you!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ich liebe dich . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzEpesoInDY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9af664-0a55-4693-987a-92742752164a"
      },
      "source": [
        "translate(\"I want to kiss you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ich mochte dich kussen . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdKsAvQx8Be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a920db1-0929-42b6-9943-0aa9f6d193d8"
      },
      "source": [
        "translate(\"I played the piano today\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ich habe heute ein klavier gespielt . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myD1vyw98Wcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ec5f042-d088-4bc2-9bc7-3ebf8c868ab3"
      },
      "source": [
        "translate(\"The teacher was happy to train the language model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['der lehrer war glucklich . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDZeYQm1OYnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63526747-7dcc-479b-ce70-2eca1eeecf26"
      },
      "source": [
        "bleu_scores = []\n",
        "smooth_fn = SmoothingFunction()\n",
        "\n",
        "for input, target in zip(X_test, Y_test):\n",
        "    original = ge_tokenizer.sequences_to_texts([target])[0]\n",
        "    predicted = translate([input], preprocess=False)\n",
        "    original = re.sub(\"(<end>)|(<start>)|\\?|!|\\.\", \"\", original)\n",
        "    predicted = re.sub(\"(<end>)|\\?|!|\\.\", \"\", predicted)\n",
        "    original_tokens = original.strip().split(\" \")\n",
        "    predicted_tokens = predicted.strip().split(\" \")\n",
        "    score = sentence_bleu([original_tokens], predicted_tokens, \n",
        "                          smoothing_function=smooth_fn.method1)\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "np.mean(np.array(bleu_scores)) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.038424524712184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOhJbErlfSV0"
      },
      "source": [
        "IMPORTANT: Such complex models **need** an adaptive learning rate! Also the hyperparameters have to be tuned according to the task. In this course, we are not implementing them, but you should definitely play with them to improve your model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NN227QxT3Lc"
      },
      "source": [
        "Beam search can be very helpful to achieve a better BLEU score. This can be implemented with the tfa.seq2seq.BeamSearchDecoder module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VV5Stos-lL_"
      },
      "source": [
        "# Continue Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypxQZyONBR6a"
      },
      "source": [
        "This time the extra code is waaay easier than the course code! :D This will definitely help you understand the previous code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYl0rDvD-qOl"
      },
      "source": [
        "## Date Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuZYLHea-ogH"
      },
      "source": [
        "Train an seq2seq model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\"). We use character-level translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC6ZqrLtAT7F"
      },
      "source": [
        "Note: Of course this can be simply done with regular expressions, but let's make a neural network learn the rules only from data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQilGMdt-m-p"
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "# Create random dates\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obsGNupu-4oe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a31b9f54-5b01-4b09-8c8c-e644fe746144"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "x_example, y_example = random_dates(3)\n",
        "for idx in range(3):\n",
        "    print(f\"Input: {x_example[idx]}, Target: {y_example[idx]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: September 20, 7075, Target: 7075-09-20\n",
            "Input: May 15, 8579, Target: 8579-05-15\n",
            "Input: January 11, 7103, Target: 7103-01-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVcNoexwBAto"
      },
      "source": [
        "Preprocessing (Always the most tedious part...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixjk26pjBD1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bade8512-4694-47b2-c21b-00dae240bcf2"
      },
      "source": [
        "# create mapping from vocab chars to ints\n",
        "input_vocab = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"01234567890, \"\n",
        "output_vocab = \"0123456789-\"\n",
        "input_vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ADFJMNOSabceghilmnoprstuvy01234567890, '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCp3lvfjDyFL"
      },
      "source": [
        "input_char2id = {c:i for i, c in enumerate(input_vocab)}\n",
        "output_char2id = {c:i for i, c in enumerate(output_vocab)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60HRdmOIESBU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4b53006-5b5b-431f-ea55-6d2ff00c1f6a"
      },
      "source": [
        "print([input_char2id[x] + 1 for x in x_example[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 12, 20, 23, 12, 17, 10, 12, 21, 39, 29, 37, 38, 39, 34, 37, 34, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyQAmVGeHUcK"
      },
      "source": [
        "Let's write a function that converts date strings to integers. We want to add padding for input strings, to have same length inputs. **ALWAYS use 0 as the padding ID**\n",
        "\n",
        "What is the maximal input length? \n",
        "\n",
        "September xx, xxxx : 18 characters\n",
        "\n",
        "What is the output length? xxxx-xx-xx : 10 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrlVKlysH8YJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3903b870-e002-47c6-c469-55053c1095fe"
      },
      "source": [
        "def string_to_char(data_str, vocabulary, max_length=None):\n",
        "    if max_length:\n",
        "        ids = [vocabulary[character] + 1 for character in data_str] # we add one to have 0 as a padding\n",
        "        for i in range(max_length - len(ids)):\n",
        "          ids.append(0)\n",
        "    else:\n",
        "        ids = [vocabulary[character] for character in data_str]\n",
        "    return np.array(ids)\n",
        "\n",
        "max_input_length = 18\n",
        "max_output_length = 10     \n",
        "string_to_char(x_example[1], input_char2id, max_length=max_input_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5,  9, 26, 39, 28, 32, 38, 39, 35, 32, 34, 36,  0,  0,  0,  0,  0,\n",
              "        0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdHR77b6E0Yg"
      },
      "source": [
        "def create_dataset(n_dates):\n",
        "    x_strings, y_strings = random_dates(n_dates)\n",
        "    x_ids = []\n",
        "    for x in x_strings:\n",
        "        x_ids.append(string_to_char(x, input_char2id, max_length=max_input_length))\n",
        "    y_ids = []\n",
        "    for y in y_strings:\n",
        "        y_ids.append(string_to_char(y, output_char2id))\n",
        "    return tf.convert_to_tensor(x_ids), tf.convert_to_tensor(y_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntyU7iTuE_J8"
      },
      "source": [
        "X_train, y_train = create_dataset(10000)\n",
        "X_valid, y_valid = create_dataset(2000)\n",
        "X_test, y_test = create_dataset(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOw6LQTU40en",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c328d53-5d07-4b63-87cf-257077388522"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 7,  3,  7,  2, 10,  1,  1, 10,  2,  5], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6doDvnpPcw_"
      },
      "source": [
        "Finally the model part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAGQ5IKYdlEu"
      },
      "source": [
        "**First Version** let's try a very basic Seq2Seq model without teaching forcing (The Decoder never knows the real inputs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msaegZEJYR2I"
      },
      "source": [
        "embedding_size = 32\n",
        "batch_size = 64\n",
        "hidden_units = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsn3hjtYVBmV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "ed85c8cc-acec-4ebf-871e-2f1402a06e23"
      },
      "source": [
        "# ENCODER\n",
        "encoder = tf.keras.models.Sequential()\n",
        "encoder.add(tf.keras.layers.Embedding(input_dim=len(input_vocab) + 1,\n",
        "                           output_dim=embedding_size,\n",
        "                           input_shape=[None]))\n",
        "encoder.add(tf.keras.layers.LSTM(hidden_units))\n",
        "\n",
        "# DECODER\n",
        "decoder = tf.keras.models.Sequential()\n",
        "decoder.add(tf.keras.layers.LSTM(hidden_units, return_sequences=True))\n",
        "decoder.add(tf.keras.layers.Dense(len(output_vocab) + 1, activation=\"softmax\"))\n",
        "\n",
        "# ENCODER-DECODER\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(encoder)\n",
        "model.add(tf.keras.layers.RepeatVector(max_output_length)) # The decoder receives the hidden state from encoder\n",
        "model.add(decoder)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - 10s 61ms/step - loss: 2.0084 - accuracy: 0.2852 - val_loss: 1.5278 - val_accuracy: 0.4218\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 9s 56ms/step - loss: 1.3572 - accuracy: 0.4958 - val_loss: 1.2038 - val_accuracy: 0.5587\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 9s 56ms/step - loss: 1.3416 - accuracy: 0.5172 - val_loss: 1.1503 - val_accuracy: 0.5774\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 9s 57ms/step - loss: 1.0726 - accuracy: 0.6014 - val_loss: 0.9961 - val_accuracy: 0.6345\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 9s 56ms/step - loss: 0.9135 - accuracy: 0.6621 - val_loss: 0.8414 - val_accuracy: 0.6842\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 9s 57ms/step - loss: 0.7682 - accuracy: 0.7102 - val_loss: 0.7065 - val_accuracy: 0.7297\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 9s 56ms/step - loss: 0.6497 - accuracy: 0.7495 - val_loss: 0.6085 - val_accuracy: 0.7624\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 9s 55ms/step - loss: 0.5413 - accuracy: 0.7862 - val_loss: 0.4993 - val_accuracy: 0.8030\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 9s 54ms/step - loss: 0.4319 - accuracy: 0.8281 - val_loss: 0.3881 - val_accuracy: 0.8432\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.3894 - accuracy: 0.8501 - val_loss: 0.3262 - val_accuracy: 0.8680\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 8s 53ms/step - loss: 0.2829 - accuracy: 0.8934 - val_loss: 0.2595 - val_accuracy: 0.9028\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.2242 - accuracy: 0.9237 - val_loss: 0.2041 - val_accuracy: 0.9327\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.1744 - accuracy: 0.9468 - val_loss: 0.1581 - val_accuracy: 0.9521\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.1309 - accuracy: 0.9649 - val_loss: 0.1184 - val_accuracy: 0.9676\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.1025 - accuracy: 0.9753 - val_loss: 0.0908 - val_accuracy: 0.9787\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0695 - accuracy: 0.9863 - val_loss: 0.0629 - val_accuracy: 0.9873\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0483 - accuracy: 0.9926 - val_loss: 0.0440 - val_accuracy: 0.9940\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0434 - accuracy: 0.9933 - val_loss: 0.0337 - val_accuracy: 0.9959\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0258 - accuracy: 0.9977 - val_loss: 0.0256 - val_accuracy: 0.9971\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0192 - accuracy: 0.9988 - val_loss: 0.0195 - val_accuracy: 0.9980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL9ey7RVF3qE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "14f10dd2-4dfa-4094-bdde-06aaac87d4bb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_12 (Sequential)   (None, 128)               83712     \n",
            "_________________________________________________________________\n",
            "repeat_vector_4 (RepeatVecto (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "sequential_13 (Sequential)   (None, 10, 12)            133132    \n",
            "=================================================================\n",
            "Total params: 216,844\n",
            "Trainable params: 216,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeQA11lhm3F8"
      },
      "source": [
        "Let's use the model to make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDeeg2h7ltRF"
      },
      "source": [
        "date = \"July 14, 1789\"\n",
        "date_int = string_to_char(date, input_char2id, max_length=max_input_length)\n",
        "date_tensor = tf.convert_to_tensor([date_int]) # It has to be in a list, since the input is a Tensor list of inputs\n",
        "prediction = np.argmax(model.predict(date_tensor), axis=-1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wQJpePU-sb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "510847ba-19b5-46a5-997b-38705effb321"
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rhWlapj9cM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab641f58-d215-4c0f-d675-06cf54c9181e"
      },
      "source": [
        "\"\".join([output_vocab[x] for x in prediction[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1789-07-14'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRhl6AzhCQuB"
      },
      "source": [
        "**Second Version** Let's try a more advanced model (for the sake of learning purposes)\n",
        "\n",
        "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qj1Bj1CC3Iq"
      },
      "source": [
        "Now let's create the decoder's inputs (for training, validation and testing). The sos (start of sentence) token will be represented using the last possible output character's ID + 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZjMe81LC46z"
      },
      "source": [
        "sos_id = len(output_vocab) + 1\n",
        "\n",
        "def shifted_output_sequences(Y):\n",
        "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
        "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
        "\n",
        "X_train_decoder = shifted_output_sequences(y_train)\n",
        "X_valid_decoder = shifted_output_sequences(y_valid)\n",
        "X_test_decoder = shifted_output_sequences(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VleKquQoDDUj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2cf6a47b-d3f8-46a6-d873-377f464f06df"
      },
      "source": [
        "X_train_decoder[0] # 12 is the SOS "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([12,  7,  3,  7,  2, 10,  1,  1, 10,  2], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxAM3TLPETCu"
      },
      "source": [
        "It's not a simple sequential model anymore, it's time to use the functional API. We need an Input Layer.\n",
        "\n",
        "A LSTM layer can return its final internal states. The returned states can be used to resume the LSTM execution later, or to initialize another LSTM. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OZccmeuLM2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "96181c3e-0657-498f-fb2f-b4e6af581951"
      },
      "source": [
        "# ENCODER\n",
        "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = tf.keras.layers.Embedding(\n",
        "                                input_dim=len(input_vocab) + 1,\n",
        "                                output_dim=embedding_size)(encoder_input)\n",
        "\n",
        "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(\n",
        "                                hidden_units, return_state=True)(encoder_embedding)\n",
        "\n",
        "# DECODER\n",
        "decoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "decoder_embedding = tf.keras.layers.Embedding(\n",
        "                                input_dim=len(output_vocab) + 2, # SOS and EOS\n",
        "                                output_dim=embedding_size)(decoder_input)\n",
        "\n",
        "decoder_lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True)(\n",
        "                                        decoder_embedding, initial_state=[encoder_state_h, encoder_state_c])\n",
        "decoder_output = tf.keras.layers.Dense(len(output_vocab) + 1,\n",
        "                                    activation=\"softmax\")(decoder_lstm)\n",
        "\n",
        "# ENCODER-DECODER\n",
        "model = tf.keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], y_train, epochs=10,\n",
        "                    validation_data=([X_valid, X_valid_decoder], y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 1.6417 - accuracy: 0.3871 - val_loss: 1.2925 - val_accuracy: 0.5105\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.9867 - accuracy: 0.6334 - val_loss: 0.6956 - val_accuracy: 0.7470\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.4869 - accuracy: 0.8295 - val_loss: 0.3162 - val_accuracy: 0.8981\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.1837 - accuracy: 0.9547 - val_loss: 0.0976 - val_accuracy: 0.9848\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0663 - accuracy: 0.9925 - val_loss: 0.0451 - val_accuracy: 0.9956\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0351 - accuracy: 0.9971 - val_loss: 0.0221 - val_accuracy: 0.9995\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0161 - accuracy: 0.9998 - val_loss: 0.0140 - val_accuracy: 0.9995\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0102 - accuracy: 0.9999 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0332 - accuracy: 0.9921 - val_loss: 0.0123 - val_accuracy: 0.9994\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcvn3fWMO9PS"
      },
      "source": [
        "Now in 8 epochs we have 100% accuracy on the validation dataset (Don't forget using early stopping!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVm222BDPKQd"
      },
      "source": [
        "**Third Version** Same as before, but using the tensorflow Decoder seq2seq module (which includes sampling, beam search and attention)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3CWuLsbDuJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "eda56473-476f-493b-ff23-72902b1ffbf5"
      },
      "source": [
        "# ENCODER (Stays the same as before)\n",
        "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = tf.keras.layers.Embedding(\n",
        "                                input_dim=len(input_vocab) + 1,\n",
        "                                output_dim=embedding_size)(encoder_input)\n",
        "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(\n",
        "                                hidden_units, return_state=True)(encoder_embedding)\n",
        "\n",
        "# DECODER \n",
        "decoder_input = tf.keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_embedding = tf.keras.layers.Embedding(\n",
        "                                input_dim=len(output_vocab) + 2, # SOS and EOS\n",
        "                                output_dim=embedding_size)(decoder_input)\n",
        "# (This part changes! A LOT) \n",
        "# Intead of using our decoder_lstm from last code, we use the BasicDecoder\n",
        "# Inputs: RNNCell (lstm,gru or rnn)\n",
        "#         Sampler - Samples from the output probability\n",
        "#         output layer\n",
        "# Outputs: Final outputs, final state, final sequence lengths\n",
        "# The last parenthesis (the function invocation) is the same as the decoder_lstm from previous code\n",
        "decoder_outputs, _, _ = tfa.seq2seq.basic_decoder.BasicDecoder(tf.keras.layers.LSTMCell(hidden_units),\n",
        "                                                 tfa.seq2seq.sampler.TrainingSampler(),\n",
        "                                                 output_layer=tf.keras.layers.Dense(len(output_vocab) + 1, activation=\"softmax\"))(\n",
        "                                                                              decoder_embedding,\n",
        "                                                                              initial_state=[encoder_state_h, encoder_state_c])\n",
        "# There are more than the RNN outputs, so choose the ones from last layer\n",
        "decoder_outputs = decoder_outputs.rnn_output\n",
        "\n",
        "# ENCODER-DECODER\n",
        "model = tf.keras.models.Model(inputs=[encoder_input, decoder_input],outputs=[decoder_outputs])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], y_train, epochs=10,\n",
        "                    validation_data=([X_valid, X_valid_decoder], y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 1.6399 - accuracy: 0.3952 - val_loss: 1.2744 - val_accuracy: 0.5420\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 1.0456 - accuracy: 0.6186 - val_loss: 0.8166 - val_accuracy: 0.7085\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.6225 - accuracy: 0.7809 - val_loss: 0.4173 - val_accuracy: 0.8629\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.2714 - accuracy: 0.9233 - val_loss: 0.1520 - val_accuracy: 0.9703\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.1027 - accuracy: 0.9840 - val_loss: 0.0668 - val_accuracy: 0.9933\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0467 - accuracy: 0.9967 - val_loss: 0.0358 - val_accuracy: 0.9984\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0270 - accuracy: 0.9988 - val_loss: 0.0585 - val_accuracy: 0.9852\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0147 - accuracy: 0.9997 - val_loss: 0.0115 - val_accuracy: 0.9999\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 0.9999\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0311 - accuracy: 0.9930 - val_loss: 0.0093 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcuYMLxMhEJI"
      },
      "source": [
        "**Fourth Version** This is a task for you. Add attention using Subclass API and tfa.seq2seq.AttentionWrapper. Similar to the translation example from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_RAEFxj5X7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7U_kV-ifY5X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}