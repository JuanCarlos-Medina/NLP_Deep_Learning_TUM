{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6_KPE_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ekATF6xD9gxS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_k5lKHkhVe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekATF6xD9gxS"
      },
      "source": [
        "# Key Phrase Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA4llV3WaEt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6272a5-9383-4451-8729-119740f68934"
      },
      "source": [
        "!pip install textacy==0.9.1\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import textacy.ke\n",
        "from textacy import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textacy==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/5e/3b8391cf6ff39350b73f8421184cf6792002b5c2c17982b7c9fbd5ff36de/textacy-0.9.1-py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.7MB/s \n",
            "\u001b[?25hCollecting cytoolz>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/67/1c60da8ba831bfefedb64c78b9f6820bdf58972797c95644ee3191daf27a/cytoolz-0.11.0.tar.gz (477kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (2.23.0)\n",
            "Collecting jellyfish>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/09/927ae35fc5a9f70abb6cc2c27ee88fc48549f7bc4786c1d4b177c22e997d/jellyfish-0.8.2-cp36-cp36m-manylinux2014_x86_64.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (4.1.1)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (0.17.0)\n",
            "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (0.22.2.post1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (2.5)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (1.0.2)\n",
            "Requirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python3.6/dist-packages (from textacy==0.9.1) (2.2.4)\n",
            "Collecting pyphen>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy==0.9.1) (0.11.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy==0.9.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy==0.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy==0.9.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy==0.9.1) (1.24.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy==0.9.1) (4.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (50.3.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy==0.9.1) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.12->textacy==0.9.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.12->textacy==0.9.1) (3.3.1)\n",
            "Building wheels for collected packages: cytoolz\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp36-cp36m-linux_x86_64.whl size=1225586 sha256=02a158850ce342c6dc36f7fab5a950a90092afb6275610b85068b73cb533bd8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/32/3c/9c9926b510647cacdde744b2c7acdf1ccd5896fbb7f8d5df0c\n",
            "Successfully built cytoolz\n",
            "Installing collected packages: cytoolz, jellyfish, pyphen, textacy\n",
            "Successfully installed cytoolz-0.11.0 jellyfish-0.8.2 pyphen-0.10.0 textacy-0.9.1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txSrVmvwaWbe"
      },
      "source": [
        "#Load a spacy model, which will be used for all further processing.\n",
        "en = textacy.load_spacy_lang(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql-EBg4raxvV"
      },
      "source": [
        "text = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation. Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled 'Computing Machinery and Intelligence' which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with. Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others. A major drawback of statistical methods is that they require elaborate feature engineering. Since the early 2010s, the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOxqsdSfbJlq"
      },
      "source": [
        "doc = textacy.make_spacy_doc(text, lang=en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnJFkQ07beoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138dd43f-0778-4247-8f25-72583cdb8202"
      },
      "source": [
        "textacy.ke.textrank(doc, topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('statistical natural language processing', 0.035339725905947916),\n",
              " ('natural language processing system', 0.03163262196028641),\n",
              " ('natural language task', 0.029347081246597313),\n",
              " ('natural language datum', 0.025156505672350015),\n",
              " ('style machine learning method', 0.024700572677656928)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2sjt5rKjv1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d509204-6aef-47a1-d4f6-9e5fe4ae9da8"
      },
      "source": [
        "textacy.ke.sgrank(doc, topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('natural language processing', 0.5730151310757177),\n",
              " ('natural language understanding', 0.09846370418298048),\n",
              " ('NLP', 0.01582432031799395),\n",
              " ('artificial intelligence', 0.014770636080973837),\n",
              " ('deep neural network', 0.012977602491495587)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN38JxbNbl9S"
      },
      "source": [
        "To address the issue of overlapping key phrases, textacy has a function: aggregage_term_variants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A24eLHiMbfuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6059fe77-333b-4f0a-e3ef-e43643f953af"
      },
      "source": [
        "terms = set([term for term, weight in textacy.ke.sgrank(doc)])\n",
        "print(textacy.ke.utils.aggregate_term_variants(terms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'natural language understanding'}, {'natural language processing'}, {'artificial intelligence'}, {'deep neural network'}, {'linguistic'}, {'computer'}, {'speech'}, {'datum'}, {'task'}, {'NLP'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TucS48A1gQyf"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0uPK5s5RzaQ"
      },
      "source": [
        "We will leave bulding a QA system in a couple of labs after learning about BERT and other transformers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0b0xrjYSIRV"
      },
      "source": [
        "For today, we will only apply the preprocessing steps for Squad v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mdzSNSUA8B_"
      },
      "source": [
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = tf.keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "eval_path = tf.keras.utils.get_file(\"eval.json\", eval_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQuTYdCZK0XI"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmNiEMqoLkDS",
        "outputId": "942143c3-cedd-4a10-a6b4-534a2631d390"
      },
      "source": [
        "raw_train_data[\"data\"][0][\"paragraphs\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'qas': [{'answers': [{'answer_start': 515,\n",
              "     'text': 'Saint Bernadette Soubirous'}],\n",
              "   'id': '5733be284776f41900661182',\n",
              "   'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'},\n",
              "  {'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}],\n",
              "   'id': '5733be284776f4190066117f',\n",
              "   'question': 'What is in front of the Notre Dame Main Building?'},\n",
              "  {'answers': [{'answer_start': 279, 'text': 'the Main Building'}],\n",
              "   'id': '5733be284776f41900661180',\n",
              "   'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?'},\n",
              "  {'answers': [{'answer_start': 381,\n",
              "     'text': 'a Marian place of prayer and reflection'}],\n",
              "   'id': '5733be284776f41900661181',\n",
              "   'question': 'What is the Grotto at Notre Dame?'},\n",
              "  {'answers': [{'answer_start': 92,\n",
              "     'text': 'a golden statue of the Virgin Mary'}],\n",
              "   'id': '5733be284776f4190066117e',\n",
              "   'question': 'What sits on top of the Main Building at Notre Dame?'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEWGLC0eDQFw"
      },
      "source": [
        "Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANpYSFfnPuyX"
      },
      "source": [
        "all_text = []\n",
        "\n",
        "for item in raw_train_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            all_text.append(para[\"context\"])\n",
        "            for qa in para[\"qas\"]:\n",
        "                all_text.append([qa[\"question\"]])\n",
        "\n",
        "for item in raw_test_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            all_text.append(para[\"context\"])\n",
        "            for qa in para[\"qas\"]:\n",
        "                all_text.append([qa[\"question\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4VQGYcPsWq"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(all_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6rhMQkVRh4o",
        "outputId": "e771bcc4-d2d8-4621-a830-022af6f54dd8"
      },
      "source": [
        "print(all_text[2])\n",
        "tokenizer.texts_to_sequences(all_text[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What is in front of the Notre Dame Main Building?']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[244, 8, 4, 1022, 2, 1, 2567, 2568, 274, 327]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JorOr2iKT6T"
      },
      "source": [
        "max_len = 384 # context + question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0XA98CuVty4"
      },
      "source": [
        "Squad Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afFlI7gz9YKm"
      },
      "source": [
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Find end character index of answer in context\n",
        "        end_char_idx = start_char_idx + len(answer_text)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in context that are in answer\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_before = tokenizer.texts_to_sequences([context[: start_char_idx]])[0]\n",
        "        tokenized_answer = tokenizer.texts_to_sequences([context[start_char_idx: end_char_idx]])[0]\n",
        "        tokenized_after = tokenizer.texts_to_sequences([context[end_char_idx:]])[0]\n",
        "        tokenized_context = tokenized_before + tokenized_answer + tokenized_after\n",
        "\n",
        "\n",
        "        # Find tokens that were created from answer characters\n",
        "        ans_token_idx = list(range(len(tokenized_before), len(tokenized_before)+ len(tokenized_answer)))\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer.texts_to_sequences([question])[0]\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_context  + tokenized_question\n",
        "        token_type_ids = [0] * len(tokenized_context) + [1] * len(tokenized_question)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxcACDdyJlds"
      },
      "source": [
        "Important: We are missing \\<start> \\<end> tokens. Transformer tokenizers add them automatically, that is why we don't do it here. However, if you want to train a seq2seq model don't forget them.\n",
        "\n",
        "Input: \\<start> context \\<end> question \\<end>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZX_9LOqLjNQ"
      },
      "source": [
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                squad_eg = SquadExample(question, context, start_char_idx, answer_text)\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdYN3p5KRHxn"
      },
      "source": [
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8oA02AR0dd"
      },
      "source": [
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_QjtlxhTq3w",
        "outputId": "c6848226-e3d9-4f53-d72c-066baead0c59"
      },
      "source": [
        "x_train[2][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs2xeaefUXM8",
        "outputId": "595a4615-dd75-4dab-d8c9-078b74cd406e"
      },
      "source": [
        "y_train[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j00Ze_qreeKq"
      },
      "source": [
        "# Continue Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqYFl9VVeiCz"
      },
      "source": [
        "## Implement a POS-Tagger from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZsIINK2il_D"
      },
      "source": [
        "We will use the Penn Treebank dataset from NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNEhlLwikfE"
      },
      "source": [
        "The model will take a sequence of words in a sentence as input, then will output the\n",
        "corresponding POS tag for each word. Thus, for an input sequence consisting of the\n",
        "words [The, cat, sat. on, the, mat, .], the output sequence should be the POS symbols\n",
        "[DT, NN, VB, IN, DT, NN, .]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnGywqNeoEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce6013c-c4ff-4a8f-9b7c-65fd2dd235e0"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"treebank\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i9ufoyKlkqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65090db2-b9ac-4c90-e6d7-fa6f78a577b3"
      },
      "source": [
        "sentences = nltk.corpus.treebank.tagged_sents()\n",
        "sents = []\n",
        "poss = []\n",
        "for sentence in sentences:\n",
        "    sents.append(\" \".join([w for w, p in sentence]))\n",
        "    poss.append(\" \".join([p for w, p in sentence]))\n",
        "\n",
        "print(len(sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5IjJsBbmV-h"
      },
      "source": [
        "sent_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "sent_tokenizer.fit_on_texts(sents)\n",
        "\n",
        "poss_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
        "poss_tokenizer.fit_on_texts(poss)\n",
        "\n",
        "sent_vocab_size = len(sent_tokenizer.word_index)\n",
        "poss_vocab_size = len(poss_tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EjGxH7euaOA"
      },
      "source": [
        "def max_len(sentences):\n",
        "    return max(len(s) for s in sentences)\n",
        "\n",
        "sents_sequences = sent_tokenizer.texts_to_sequences(sents)\n",
        "max_seqlen = max_len(sents_sequences)\n",
        "\n",
        "sents_sequences = tf.keras.preprocessing.sequence.pad_sequences(sents_sequences,\n",
        "                                maxlen=max_seqlen, padding=\"post\")\n",
        "poss_sequences = poss_tokenizer.texts_to_sequences(poss)\n",
        "poss_sequences = tf.keras.preprocessing.sequence.pad_sequences(poss_sequences,\n",
        "                                maxlen=max_seqlen, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC6W8j7zrGbl"
      },
      "source": [
        "This time we will preprocess the POS tags not as a sequence but as categories! This makes the problem simpler. We will treat is as a multiclassification problem instead of a seq2seq problem. (A seq2seq can have better accuracy as it takes into account the correct order of POS tags, but lets keep it simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFc6rBH-p1BA"
      },
      "source": [
        "poss_categories = []\n",
        "for p in poss_sequences:\n",
        "    poss_categories.append(tf.keras.utils.to_categorical(p, num_classes=poss_vocab_size+1, dtype=\"int32\"))\n",
        "poss_categories = tf.keras.preprocessing.sequence.pad_sequences(poss_categories, maxlen=max_seqlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyACRhofCBTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da992b9c-7cea-41a1-d15f-01360d70d7d9"
      },
      "source": [
        "print(sents_sequences[0])\n",
        "print(poss_sequences[0])\n",
        "print(poss_categories[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5601 3746    1 2024   86  331    1   46 2405    2  131   27    6 2025\n",
            "  332  459 2026    3    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 3  3  8 10  6  7  8 21 13  4  1  2  4  7  1  3 10  9  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poYnUzBwB5T9"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((sents_sequences, poss_categories))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuFbcZwmrwVc"
      },
      "source": [
        "Instead of using scikit learn train, split code, we will apply directly Dataset API operations. It is quite straightforward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHd7k4serqo1"
      },
      "source": [
        "# split into training, validation, and test datasets\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = len(sents) // 3\n",
        "val_size = (len(sents) - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIYx6F3GYDmG"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIVisXPXC50Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2f1216-e3cc-4a54-81ea-7fb1c65ae598"
      },
      "source": [
        "embedding_dims = 128\n",
        "hidden_units = 256\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(sent_vocab_size + 1, # Add Padding!\n",
        "                                    embedding_dims, \n",
        "                                    input_length=max_seqlen))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_units, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dense(poss_vocab_size + 1, activation=\"softmax\"))  # Add Padding!\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 271, 128)          1457664   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 271, 128)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 271, 512)          592896    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 271, 47)           24111     \n",
            "=================================================================\n",
            "Total params: 2,074,671\n",
            "Trainable params: 2,074,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9w8RKkrZP1G"
      },
      "source": [
        "Because of the padding, there are a lot of zeros on both the label and prediction, as a result of which the normal accuracy numbers will be very optimistic. Let's implement a maske accuracy that does not take into account the zero labels.\n",
        "\n",
        "Very similar to the loss we implemented in the machine translation code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agofDQEAdnd9"
      },
      "source": [
        "However, this time I use tf.keras.backend to do operations between Tensors. They are almost the same as tf direct operations (tf.argmax is equivalent to tf.keras.backend.argmax. \n",
        "BUT, not always! (tf.keras.backend.sum is equivalent to tf.reduce_sum)\n",
        "\n",
        "I show it here for you not to get confused when seeing the keras.backend operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LCgSWyOakuh"
      },
      "source": [
        "def masked_accuracy():\n",
        "    def masked_accuracy_fn(ytrue, ypred):\n",
        "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
        "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
        " \n",
        "        mask = tf.keras.backend.cast(\n",
        "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
        "        matches = tf.keras.backend.cast(\n",
        "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
        "        numer = tf.keras.backend.sum(matches)\n",
        "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
        "        accuracy =  numer / denom\n",
        "        return accuracy\n",
        "\n",
        "    return masked_accuracy_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRYiC8FnGaZU"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\", masked_accuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxOFnJLJGfJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1840f8-c524-4782-a618-0a0b4bdb4d1c"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "history = model.fit(train_dataset.batch(BATCH_SIZE), \n",
        "                    epochs=20,\n",
        "                    validation_data=val_dataset.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "74/74 [==============================] - 5s 67ms/step - loss: 0.6731 - accuracy: 0.8974 - masked_accuracy_fn: 0.0991 - val_loss: 0.3185 - val_accuracy: 0.9118 - val_masked_accuracy_fn: 0.0767\n",
            "Epoch 2/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.2877 - accuracy: 0.9235 - masked_accuracy_fn: 0.2146 - val_loss: 0.2446 - val_accuracy: 0.9399 - val_masked_accuracy_fn: 0.3427\n",
            "Epoch 3/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.1951 - accuracy: 0.9508 - masked_accuracy_fn: 0.4865 - val_loss: 0.1367 - val_accuracy: 0.9653 - val_masked_accuracy_fn: 0.6348\n",
            "Epoch 4/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.1003 - accuracy: 0.9762 - masked_accuracy_fn: 0.7500 - val_loss: 0.0643 - val_accuracy: 0.9865 - val_masked_accuracy_fn: 0.8619\n",
            "Epoch 5/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0519 - accuracy: 0.9886 - masked_accuracy_fn: 0.8810 - val_loss: 0.0365 - val_accuracy: 0.9921 - val_masked_accuracy_fn: 0.9192\n",
            "Epoch 6/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.0312 - accuracy: 0.9930 - masked_accuracy_fn: 0.9273 - val_loss: 0.0235 - val_accuracy: 0.9943 - val_masked_accuracy_fn: 0.9469\n",
            "Epoch 7/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0216 - accuracy: 0.9950 - masked_accuracy_fn: 0.9470 - val_loss: 0.0141 - val_accuracy: 0.9967 - val_masked_accuracy_fn: 0.9656\n",
            "Epoch 8/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.0159 - accuracy: 0.9961 - masked_accuracy_fn: 0.9596 - val_loss: 0.0122 - val_accuracy: 0.9975 - val_masked_accuracy_fn: 0.9721\n",
            "Epoch 9/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0129 - accuracy: 0.9968 - masked_accuracy_fn: 0.9655 - val_loss: 0.0095 - val_accuracy: 0.9976 - val_masked_accuracy_fn: 0.9747\n",
            "Epoch 10/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0106 - accuracy: 0.9973 - masked_accuracy_fn: 0.9714 - val_loss: 0.0077 - val_accuracy: 0.9981 - val_masked_accuracy_fn: 0.9799\n",
            "Epoch 11/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0092 - accuracy: 0.9975 - masked_accuracy_fn: 0.9738 - val_loss: 0.0071 - val_accuracy: 0.9982 - val_masked_accuracy_fn: 0.9819\n",
            "Epoch 12/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0079 - accuracy: 0.9978 - masked_accuracy_fn: 0.9770 - val_loss: 0.0068 - val_accuracy: 0.9979 - val_masked_accuracy_fn: 0.9775\n",
            "Epoch 13/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0074 - accuracy: 0.9980 - masked_accuracy_fn: 0.9787 - val_loss: 0.0066 - val_accuracy: 0.9983 - val_masked_accuracy_fn: 0.9804\n",
            "Epoch 14/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0065 - accuracy: 0.9982 - masked_accuracy_fn: 0.9806 - val_loss: 0.0044 - val_accuracy: 0.9988 - val_masked_accuracy_fn: 0.9881\n",
            "Epoch 15/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.0063 - accuracy: 0.9982 - masked_accuracy_fn: 0.9812 - val_loss: 0.0043 - val_accuracy: 0.9988 - val_masked_accuracy_fn: 0.9862\n",
            "Epoch 16/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0053 - accuracy: 0.9986 - masked_accuracy_fn: 0.9849 - val_loss: 0.0041 - val_accuracy: 0.9989 - val_masked_accuracy_fn: 0.9872\n",
            "Epoch 17/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.0049 - accuracy: 0.9986 - masked_accuracy_fn: 0.9858 - val_loss: 0.0035 - val_accuracy: 0.9992 - val_masked_accuracy_fn: 0.9921\n",
            "Epoch 18/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0046 - accuracy: 0.9987 - masked_accuracy_fn: 0.9864 - val_loss: 0.0036 - val_accuracy: 0.9990 - val_masked_accuracy_fn: 0.9891\n",
            "Epoch 19/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.0042 - accuracy: 0.9988 - masked_accuracy_fn: 0.9875 - val_loss: 0.0030 - val_accuracy: 0.9992 - val_masked_accuracy_fn: 0.9924\n",
            "Epoch 20/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.0038 - accuracy: 0.9989 - masked_accuracy_fn: 0.9888 - val_loss: 0.0026 - val_accuracy: 0.9993 - val_masked_accuracy_fn: 0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD5YzxxIdUW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ecbdd2-3595-4f52-f754-9aa7a795790b"
      },
      "source": [
        "model.evaluate(test_dataset.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41/41 [==============================] - 1s 17ms/step - loss: 0.0027 - accuracy: 0.9993 - masked_accuracy_fn: 0.9922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0026725211646407843, 0.9992699027061462, 0.9921793937683105]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivVXbXxye7i4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2121f78-d523-4ff1-f70d-dd8e91d3fbde"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for test_example in test_dataset.take(5).batch(5):\n",
        "    input, output = test_example\n",
        "    pred = model.predict(input)\n",
        "    preds_b = np.argmax(pred, axis=-1)\n",
        "    outputs_b = np.argmax(output.numpy(), axis=-1)\n",
        "    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n",
        "        input_tokens = sent_tokenizer.sequences_to_texts(input.numpy())[0].split(\" \")\n",
        "        output_tokens = poss_tokenizer.sequences_to_texts([output_l])[0].split(\" \")\n",
        "        predicted_tokens = poss_tokenizer.sequences_to_texts([pred_l])[0].split(\" \")\n",
        "        true = \"\"\n",
        "        predicted = \"\"\n",
        "        for i, o, p in zip(input_tokens, output_tokens, predicted_tokens):\n",
        "          true = true + i + \"/\" + o + \" \"\n",
        "          predicted = predicted + i + \"/\" + p + \" \"\n",
        "        print(\"True:\", true.strip())\n",
        "        print(\"Predicted:\", predicted.strip())\n",
        "        print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True: that/DT explains/VBZ why/WRB the/DT number/NN of/IN these/DT wines/NNS is/VBZ expanding/VBG so/RB rapidly/RB ./.\n",
            "Predicted: that/DT explains/VBZ why/WRB the/DT number/NN of/IN these/DT wines/NNS is/VBZ expanding/VBG so/RB rapidly/RB ./.\n",
            "\n",
            "\n",
            "\n",
            "True: that/PRP explains/VBD why/NNS the/IN number/DT of/JJ these/NN wines/DT is/NN expanding/RB so/WP rapidly/-NONE- ./VBD\n",
            "Predicted: that/PRP explains/VBD why/NNS the/IN number/DT of/JJ these/NN wines/DT is/NN expanding/JJR so/WP rapidly/-NONE- ./VBD\n",
            "\n",
            "\n",
            "\n",
            "True: that/JJ explains/NN why/VBN the/-NONE- number/RB of/IN these/NNP wines/NNPS is/NNP expanding/NNP so/: rapidly/CD ./NN\n",
            "Predicted: that/JJ explains/NN why/VBN the/-NONE- number/RB of/IN these/NNP wines/NNPS is/NNP expanding/NNP so/: rapidly/CD ./NN\n",
            "\n",
            "\n",
            "\n",
            "True: that/`` explains/PRP why/VBP the/VBN number/-NONE- of/TO these/VB wines/DT is/NN expanding/NN so/, rapidly/'' ./VBD\n",
            "Predicted: that/`` explains/PRP why/VBP the/VBN number/-NONE- of/TO these/VB wines/DT is/NN expanding/NN so/, rapidly/'' ./VBD\n",
            "\n",
            "\n",
            "\n",
            "True: that/NN explains/RB why/VBZ the/PRP$ number/NN of/NN these/IN wines/NNP is/TO expanding/NNP so/NNP rapidly/, ./IN\n",
            "Predicted: that/NN explains/RB why/VBZ the/PRP$ number/NN of/NN these/IN wines/NNP is/TO expanding/NNP so/NNP rapidly/, ./IN\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}